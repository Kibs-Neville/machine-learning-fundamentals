2025-06-03 10:54:32,950:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 10:54:32,952:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 10:54:32,952:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 10:54:32,952:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 10:59:53,541:INFO:PyCaret ClassificationExperiment
2025-06-03 10:59:53,545:INFO:Logging name: clf-default-name
2025-06-03 10:59:53,545:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-03 10:59:53,547:INFO:version 3.3.2
2025-06-03 10:59:53,547:INFO:Initializing setup()
2025-06-03 10:59:53,547:INFO:self.USI: 13d0
2025-06-03 10:59:53,549:INFO:self._variable_keys: {'fix_imbalance', 'is_multiclass', 'X_test', 'fold_groups_param', 'USI', 'seed', 'exp_id', '_ml_usecase', 'target_param', 'gpu_n_jobs_param', 'X_train', 'fold_generator', 'y_test', 'y_train', 'html_param', 'y', '_available_plots', 'X', 'data', 'logging_param', 'log_plots_param', 'memory', 'fold_shuffle_param', 'gpu_param', 'n_jobs_param', 'pipeline', 'idx', 'exp_name_log'}
2025-06-03 10:59:53,549:INFO:Checking environment
2025-06-03 10:59:53,562:INFO:python_version: 3.10.0
2025-06-03 10:59:53,562:INFO:python_build: ('tags/v3.10.0:b494f59', 'Oct  4 2021 19:00:18')
2025-06-03 10:59:53,564:INFO:machine: AMD64
2025-06-03 10:59:53,568:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-03 10:59:53,601:INFO:Memory: svmem(total=4110286848, available=380801024, percent=90.7, used=3729485824, free=380801024)
2025-06-03 10:59:53,603:INFO:Physical Core: 4
2025-06-03 10:59:53,603:INFO:Logical Core: 4
2025-06-03 10:59:53,603:INFO:Checking libraries
2025-06-03 10:59:53,605:INFO:System:
2025-06-03 10:59:53,605:INFO:    python: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
2025-06-03 10:59:53,605:INFO:executable: c:\Program Files\Python310\python.exe
2025-06-03 10:59:53,605:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-03 10:59:53,605:INFO:PyCaret required dependencies:
2025-06-03 10:59:56,464:INFO:                 pip: 21.2.3
2025-06-03 10:59:56,464:INFO:          setuptools: 57.4.0
2025-06-03 10:59:56,466:INFO:             pycaret: 3.3.2
2025-06-03 10:59:56,466:INFO:             IPython: 8.30.0
2025-06-03 10:59:56,466:INFO:          ipywidgets: 8.1.5
2025-06-03 10:59:56,466:INFO:                tqdm: 4.67.1
2025-06-03 10:59:56,466:INFO:               numpy: 1.26.4
2025-06-03 10:59:56,466:INFO:              pandas: 2.1.4
2025-06-03 10:59:56,466:INFO:              jinja2: 3.1.4
2025-06-03 10:59:56,466:INFO:               scipy: 1.11.4
2025-06-03 10:59:56,466:INFO:              joblib: 1.3.2
2025-06-03 10:59:56,466:INFO:             sklearn: 1.4.2
2025-06-03 10:59:56,466:INFO:                pyod: 2.0.5
2025-06-03 10:59:56,468:INFO:            imblearn: 0.13.0
2025-06-03 10:59:56,468:INFO:   category_encoders: 2.7.0
2025-06-03 10:59:56,468:INFO:            lightgbm: 4.6.0
2025-06-03 10:59:56,468:INFO:               numba: 0.61.2
2025-06-03 10:59:56,468:INFO:            requests: 2.32.3
2025-06-03 10:59:56,468:INFO:          matplotlib: 3.7.5
2025-06-03 10:59:56,468:INFO:          scikitplot: 0.3.7
2025-06-03 10:59:56,468:INFO:         yellowbrick: 1.5
2025-06-03 10:59:56,468:INFO:              plotly: 5.24.1
2025-06-03 10:59:56,468:INFO:    plotly-resampler: Not installed
2025-06-03 10:59:56,468:INFO:             kaleido: 0.2.1
2025-06-03 10:59:56,468:INFO:           schemdraw: 0.15
2025-06-03 10:59:56,468:INFO:         statsmodels: 0.14.4
2025-06-03 10:59:56,470:INFO:              sktime: 0.26.0
2025-06-03 10:59:56,470:INFO:               tbats: 1.1.3
2025-06-03 10:59:56,470:INFO:            pmdarima: 2.0.4
2025-06-03 10:59:56,470:INFO:              psutil: 6.1.0
2025-06-03 10:59:56,470:INFO:          markupsafe: 3.0.2
2025-06-03 10:59:56,470:INFO:             pickle5: Not installed
2025-06-03 10:59:56,470:INFO:         cloudpickle: 3.1.1
2025-06-03 10:59:56,470:INFO:         deprecation: 2.1.0
2025-06-03 10:59:56,470:INFO:              xxhash: 3.5.0
2025-06-03 10:59:56,470:INFO:           wurlitzer: Not installed
2025-06-03 10:59:56,470:INFO:PyCaret optional dependencies:
2025-06-03 10:59:56,634:INFO:                shap: Not installed
2025-06-03 10:59:56,634:INFO:           interpret: Not installed
2025-06-03 10:59:56,634:INFO:                umap: Not installed
2025-06-03 10:59:56,634:INFO:     ydata_profiling: Not installed
2025-06-03 10:59:56,634:INFO:  explainerdashboard: Not installed
2025-06-03 10:59:56,634:INFO:             autoviz: Not installed
2025-06-03 10:59:56,634:INFO:           fairlearn: Not installed
2025-06-03 10:59:56,634:INFO:          deepchecks: Not installed
2025-06-03 10:59:56,634:INFO:             xgboost: Not installed
2025-06-03 10:59:56,634:INFO:            catboost: Not installed
2025-06-03 10:59:56,634:INFO:              kmodes: Not installed
2025-06-03 10:59:56,634:INFO:             mlxtend: Not installed
2025-06-03 10:59:56,634:INFO:       statsforecast: Not installed
2025-06-03 10:59:56,639:INFO:        tune_sklearn: Not installed
2025-06-03 10:59:56,639:INFO:                 ray: Not installed
2025-06-03 10:59:56,639:INFO:            hyperopt: Not installed
2025-06-03 10:59:56,642:INFO:              optuna: Not installed
2025-06-03 10:59:56,642:INFO:               skopt: Not installed
2025-06-03 10:59:56,642:INFO:              mlflow: Not installed
2025-06-03 10:59:56,644:INFO:              gradio: Not installed
2025-06-03 10:59:56,644:INFO:             fastapi: Not installed
2025-06-03 10:59:56,646:INFO:             uvicorn: Not installed
2025-06-03 10:59:56,650:INFO:              m2cgen: Not installed
2025-06-03 10:59:56,650:INFO:           evidently: Not installed
2025-06-03 10:59:56,650:INFO:               fugue: Not installed
2025-06-03 10:59:56,652:INFO:           streamlit: 1.44.1
2025-06-03 10:59:56,652:INFO:             prophet: Not installed
2025-06-03 10:59:56,652:INFO:None
2025-06-03 10:59:56,652:INFO:Set up data.
2025-06-03 10:59:56,801:INFO:Set up folding strategy.
2025-06-03 10:59:56,803:INFO:Set up train/test split.
2025-06-03 10:59:56,950:INFO:Set up index.
2025-06-03 10:59:56,956:INFO:Assigning column types.
2025-06-03 10:59:57,027:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-03 10:59:57,304:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 10:59:57,319:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 10:59:57,735:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:57,735:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:58,090:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 10:59:58,092:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 10:59:58,584:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:58,586:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:58,588:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-03 10:59:58,958:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 10:59:59,123:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:59,125:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:59,372:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 10:59:59,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:59,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:59,490:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-03 10:59:59,798:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 10:59:59,800:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:00,191:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:00,191:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:00,213:INFO:Preparing preprocessing pipeline...
2025-06-03 11:00:00,223:INFO:Set up simple imputation.
2025-06-03 11:00:00,472:INFO:Finished creating preprocessing pipeline.
2025-06-03 11:00:00,498:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\NEVILL~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['pixel_0_0', 'pixel_0_1',
                                             'pixel_0_2', 'pixel_0_3',
                                             'pixel_0_4', 'pixel_0_5',
                                             'pixel_0_6', 'pixel_0_7',
                                             'pixel_1_0', 'pixel_1_1',
                                             'pixel_1_2', 'pixel_1_3',
                                             'pixel_1_4', 'pixel_1_5',
                                             'pixel_1_6', 'pixel_1_7',
                                             'pixel_2...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-06-03 11:00:00,500:INFO:Creating final display dataframe.
2025-06-03 11:00:00,930:INFO:Setup _display_container:                     Description             Value
0                    Session id              8280
1                        Target            target
2                   Target type        Multiclass
3           Original data shape        (1797, 65)
4        Transformed data shape        (1797, 65)
5   Transformed train set shape        (1257, 65)
6    Transformed test set shape         (540, 65)
7              Numeric features                64
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              13d0
2025-06-03 11:00:01,349:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:01,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:01,615:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:01,615:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:00:01,641:INFO:setup() successfully completed in 8.22s...............
2025-06-03 11:00:48,562:INFO:Initializing compare_models()
2025-06-03 11:00:48,562:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-03 11:00:48,562:INFO:Checking exceptions
2025-06-03 11:00:48,626:INFO:Preparing display monitor
2025-06-03 11:00:48,845:INFO:Initializing Logistic Regression
2025-06-03 11:00:48,845:INFO:Total runtime is 0.0 minutes
2025-06-03 11:00:48,859:INFO:SubProcess create_model() called ==================================
2025-06-03 11:00:48,865:INFO:Initializing create_model()
2025-06-03 11:00:48,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:00:48,867:INFO:Checking exceptions
2025-06-03 11:00:48,869:INFO:Importing libraries
2025-06-03 11:00:48,869:INFO:Copying training dataset
2025-06-03 11:00:49,081:INFO:Defining folds
2025-06-03 11:00:49,081:INFO:Declaring metric variables
2025-06-03 11:00:49,097:INFO:Importing untrained model
2025-06-03 11:00:49,111:INFO:Logistic Regression Imported successfully
2025-06-03 11:00:49,145:INFO:Starting cross validation
2025-06-03 11:00:49,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:12,078:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:12,852:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:12,947:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:12,961:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:12,964:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:13,366:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:13,383:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:13,438:INFO:Calculating mean and std
2025-06-03 11:01:13,550:INFO:Creating metrics dataframe
2025-06-03 11:01:13,647:INFO:Uploading results into container
2025-06-03 11:01:13,660:INFO:Uploading model into container now
2025-06-03 11:01:13,667:INFO:_master_model_container: 1
2025-06-03 11:01:13,675:INFO:_display_container: 2
2025-06-03 11:01:13,675:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8280, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-03 11:01:13,675:INFO:create_model() successfully completed......................................
2025-06-03 11:01:16,354:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:16,354:INFO:Creating metrics dataframe
2025-06-03 11:01:16,385:INFO:Initializing K Neighbors Classifier
2025-06-03 11:01:16,385:INFO:Total runtime is 0.45899611711502075 minutes
2025-06-03 11:01:16,401:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:16,401:INFO:Initializing create_model()
2025-06-03 11:01:16,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:16,401:INFO:Checking exceptions
2025-06-03 11:01:16,401:INFO:Importing libraries
2025-06-03 11:01:16,401:INFO:Copying training dataset
2025-06-03 11:01:16,449:INFO:Defining folds
2025-06-03 11:01:16,449:INFO:Declaring metric variables
2025-06-03 11:01:16,467:INFO:Importing untrained model
2025-06-03 11:01:16,467:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:01:16,496:INFO:Starting cross validation
2025-06-03 11:01:16,496:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:17,614:INFO:Calculating mean and std
2025-06-03 11:01:17,614:INFO:Creating metrics dataframe
2025-06-03 11:01:17,626:INFO:Uploading results into container
2025-06-03 11:01:17,626:INFO:Uploading model into container now
2025-06-03 11:01:17,626:INFO:_master_model_container: 2
2025-06-03 11:01:17,626:INFO:_display_container: 2
2025-06-03 11:01:17,626:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:01:17,626:INFO:create_model() successfully completed......................................
2025-06-03 11:01:17,813:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:17,816:INFO:Creating metrics dataframe
2025-06-03 11:01:17,840:INFO:Initializing Naive Bayes
2025-06-03 11:01:17,840:INFO:Total runtime is 0.48323899507522583 minutes
2025-06-03 11:01:17,869:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:17,871:INFO:Initializing create_model()
2025-06-03 11:01:17,871:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:17,873:INFO:Checking exceptions
2025-06-03 11:01:17,873:INFO:Importing libraries
2025-06-03 11:01:17,873:INFO:Copying training dataset
2025-06-03 11:01:17,991:INFO:Defining folds
2025-06-03 11:01:17,991:INFO:Declaring metric variables
2025-06-03 11:01:18,021:INFO:Importing untrained model
2025-06-03 11:01:18,036:INFO:Naive Bayes Imported successfully
2025-06-03 11:01:18,107:INFO:Starting cross validation
2025-06-03 11:01:18,117:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:18,818:INFO:Calculating mean and std
2025-06-03 11:01:18,818:INFO:Creating metrics dataframe
2025-06-03 11:01:18,818:INFO:Uploading results into container
2025-06-03 11:01:18,818:INFO:Uploading model into container now
2025-06-03 11:01:18,818:INFO:_master_model_container: 3
2025-06-03 11:01:18,818:INFO:_display_container: 2
2025-06-03 11:01:18,818:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-03 11:01:18,818:INFO:create_model() successfully completed......................................
2025-06-03 11:01:19,002:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:19,005:INFO:Creating metrics dataframe
2025-06-03 11:01:19,024:INFO:Initializing Decision Tree Classifier
2025-06-03 11:01:19,024:INFO:Total runtime is 0.5029808243115743 minutes
2025-06-03 11:01:19,045:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:19,045:INFO:Initializing create_model()
2025-06-03 11:01:19,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:19,048:INFO:Checking exceptions
2025-06-03 11:01:19,050:INFO:Importing libraries
2025-06-03 11:01:19,050:INFO:Copying training dataset
2025-06-03 11:01:19,161:INFO:Defining folds
2025-06-03 11:01:19,161:INFO:Declaring metric variables
2025-06-03 11:01:19,175:INFO:Importing untrained model
2025-06-03 11:01:19,184:INFO:Decision Tree Classifier Imported successfully
2025-06-03 11:01:19,209:INFO:Starting cross validation
2025-06-03 11:01:19,213:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:20,676:INFO:Calculating mean and std
2025-06-03 11:01:20,692:INFO:Creating metrics dataframe
2025-06-03 11:01:20,703:INFO:Uploading results into container
2025-06-03 11:01:20,715:INFO:Uploading model into container now
2025-06-03 11:01:20,719:INFO:_master_model_container: 4
2025-06-03 11:01:20,719:INFO:_display_container: 2
2025-06-03 11:01:20,725:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=8280, splitter='best')
2025-06-03 11:01:20,729:INFO:create_model() successfully completed......................................
2025-06-03 11:01:21,439:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:21,439:INFO:Creating metrics dataframe
2025-06-03 11:01:21,496:INFO:Initializing SVM - Linear Kernel
2025-06-03 11:01:21,496:INFO:Total runtime is 0.5441799600919088 minutes
2025-06-03 11:01:21,516:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:21,516:INFO:Initializing create_model()
2025-06-03 11:01:21,516:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:21,516:INFO:Checking exceptions
2025-06-03 11:01:21,519:INFO:Importing libraries
2025-06-03 11:01:21,519:INFO:Copying training dataset
2025-06-03 11:01:21,656:INFO:Defining folds
2025-06-03 11:01:21,656:INFO:Declaring metric variables
2025-06-03 11:01:21,676:INFO:Importing untrained model
2025-06-03 11:01:21,709:INFO:SVM - Linear Kernel Imported successfully
2025-06-03 11:01:21,763:INFO:Starting cross validation
2025-06-03 11:01:21,767:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:22,377:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,383:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,389:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,411:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,740:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,746:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,764:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:22,775:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:23,009:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:23,017:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:23,060:INFO:Calculating mean and std
2025-06-03 11:01:23,064:INFO:Creating metrics dataframe
2025-06-03 11:01:23,064:INFO:Uploading results into container
2025-06-03 11:01:23,064:INFO:Uploading model into container now
2025-06-03 11:01:23,064:INFO:_master_model_container: 5
2025-06-03 11:01:23,064:INFO:_display_container: 2
2025-06-03 11:01:23,074:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8280, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-03 11:01:23,076:INFO:create_model() successfully completed......................................
2025-06-03 11:01:23,311:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:23,311:INFO:Creating metrics dataframe
2025-06-03 11:01:23,333:INFO:Initializing Ridge Classifier
2025-06-03 11:01:23,333:INFO:Total runtime is 0.5747933069864909 minutes
2025-06-03 11:01:23,346:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:23,349:INFO:Initializing create_model()
2025-06-03 11:01:23,349:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:23,349:INFO:Checking exceptions
2025-06-03 11:01:23,349:INFO:Importing libraries
2025-06-03 11:01:23,349:INFO:Copying training dataset
2025-06-03 11:01:23,507:INFO:Defining folds
2025-06-03 11:01:23,510:INFO:Declaring metric variables
2025-06-03 11:01:23,547:INFO:Importing untrained model
2025-06-03 11:01:23,576:INFO:Ridge Classifier Imported successfully
2025-06-03 11:01:23,620:INFO:Starting cross validation
2025-06-03 11:01:23,628:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:24,105:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,109:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,114:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,136:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,657:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,659:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,818:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:24,904:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:25,215:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:25,221:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:25,272:INFO:Calculating mean and std
2025-06-03 11:01:25,283:INFO:Creating metrics dataframe
2025-06-03 11:01:25,306:INFO:Uploading results into container
2025-06-03 11:01:25,308:INFO:Uploading model into container now
2025-06-03 11:01:25,308:INFO:_master_model_container: 6
2025-06-03 11:01:25,310:INFO:_display_container: 2
2025-06-03 11:01:25,319:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8280, solver='auto',
                tol=0.0001)
2025-06-03 11:01:25,319:INFO:create_model() successfully completed......................................
2025-06-03 11:01:25,523:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:25,523:INFO:Creating metrics dataframe
2025-06-03 11:01:25,561:INFO:Initializing Random Forest Classifier
2025-06-03 11:01:25,561:INFO:Total runtime is 0.6119346340497335 minutes
2025-06-03 11:01:25,577:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:25,577:INFO:Initializing create_model()
2025-06-03 11:01:25,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:25,577:INFO:Checking exceptions
2025-06-03 11:01:25,577:INFO:Importing libraries
2025-06-03 11:01:25,577:INFO:Copying training dataset
2025-06-03 11:01:25,656:INFO:Defining folds
2025-06-03 11:01:25,656:INFO:Declaring metric variables
2025-06-03 11:01:25,677:INFO:Importing untrained model
2025-06-03 11:01:25,695:INFO:Random Forest Classifier Imported successfully
2025-06-03 11:01:25,730:INFO:Starting cross validation
2025-06-03 11:01:25,730:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:31,070:INFO:Calculating mean and std
2025-06-03 11:01:31,072:INFO:Creating metrics dataframe
2025-06-03 11:01:31,080:INFO:Uploading results into container
2025-06-03 11:01:31,084:INFO:Uploading model into container now
2025-06-03 11:01:31,087:INFO:_master_model_container: 7
2025-06-03 11:01:31,087:INFO:_display_container: 2
2025-06-03 11:01:31,087:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8280, verbose=0,
                       warm_start=False)
2025-06-03 11:01:31,087:INFO:create_model() successfully completed......................................
2025-06-03 11:01:31,236:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:31,236:INFO:Creating metrics dataframe
2025-06-03 11:01:31,252:INFO:Initializing Quadratic Discriminant Analysis
2025-06-03 11:01:31,252:INFO:Total runtime is 0.7067802786827088 minutes
2025-06-03 11:01:31,252:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:31,252:INFO:Initializing create_model()
2025-06-03 11:01:31,252:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:31,268:INFO:Checking exceptions
2025-06-03 11:01:31,268:INFO:Importing libraries
2025-06-03 11:01:31,268:INFO:Copying training dataset
2025-06-03 11:01:31,358:INFO:Defining folds
2025-06-03 11:01:31,358:INFO:Declaring metric variables
2025-06-03 11:01:31,368:INFO:Importing untrained model
2025-06-03 11:01:31,381:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-03 11:01:31,433:INFO:Starting cross validation
2025-06-03 11:01:31,437:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:31,695:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:31,695:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:31,695:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:31,777:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:31,779:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:31,803:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:31,844:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:31,948:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,006:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,050:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,104:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,114:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,177:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,223:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,262:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,346:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,367:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:01:32,461:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,473:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:32,531:INFO:Calculating mean and std
2025-06-03 11:01:32,535:INFO:Creating metrics dataframe
2025-06-03 11:01:32,547:INFO:Uploading results into container
2025-06-03 11:01:32,549:INFO:Uploading model into container now
2025-06-03 11:01:32,551:INFO:_master_model_container: 8
2025-06-03 11:01:32,551:INFO:_display_container: 2
2025-06-03 11:01:32,551:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-03 11:01:32,553:INFO:create_model() successfully completed......................................
2025-06-03 11:01:32,784:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:32,784:INFO:Creating metrics dataframe
2025-06-03 11:01:32,823:INFO:Initializing Ada Boost Classifier
2025-06-03 11:01:32,823:INFO:Total runtime is 0.7329661369323731 minutes
2025-06-03 11:01:32,839:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:32,841:INFO:Initializing create_model()
2025-06-03 11:01:32,841:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:32,841:INFO:Checking exceptions
2025-06-03 11:01:32,841:INFO:Importing libraries
2025-06-03 11:01:32,841:INFO:Copying training dataset
2025-06-03 11:01:33,009:INFO:Defining folds
2025-06-03 11:01:33,009:INFO:Declaring metric variables
2025-06-03 11:01:33,030:INFO:Importing untrained model
2025-06-03 11:01:33,054:INFO:Ada Boost Classifier Imported successfully
2025-06-03 11:01:33,092:INFO:Starting cross validation
2025-06-03 11:01:33,097:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:33,309:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:33,316:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:33,318:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:33,318:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:34,135:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:34,168:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:34,274:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:34,279:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:34,320:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:34,326:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:34,344:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:34,361:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:34,395:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:34,433:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:34,466:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:34,485:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:35,116:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,146:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,193:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,213:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,231:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:35,263:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,273:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,283:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,291:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,306:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:01:35,794:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,806:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,830:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:35,838:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:01:35,869:INFO:Calculating mean and std
2025-06-03 11:01:35,876:INFO:Creating metrics dataframe
2025-06-03 11:01:35,884:INFO:Uploading results into container
2025-06-03 11:01:35,887:INFO:Uploading model into container now
2025-06-03 11:01:35,889:INFO:_master_model_container: 9
2025-06-03 11:01:35,889:INFO:_display_container: 2
2025-06-03 11:01:35,889:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=8280)
2025-06-03 11:01:35,894:INFO:create_model() successfully completed......................................
2025-06-03 11:01:36,124:INFO:SubProcess create_model() end ==================================
2025-06-03 11:01:36,124:INFO:Creating metrics dataframe
2025-06-03 11:01:36,163:INFO:Initializing Gradient Boosting Classifier
2025-06-03 11:01:36,163:INFO:Total runtime is 0.7886366208394369 minutes
2025-06-03 11:01:36,177:INFO:SubProcess create_model() called ==================================
2025-06-03 11:01:36,177:INFO:Initializing create_model()
2025-06-03 11:01:36,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:01:36,177:INFO:Checking exceptions
2025-06-03 11:01:36,179:INFO:Importing libraries
2025-06-03 11:01:36,179:INFO:Copying training dataset
2025-06-03 11:01:36,276:INFO:Defining folds
2025-06-03 11:01:36,276:INFO:Declaring metric variables
2025-06-03 11:01:36,305:INFO:Importing untrained model
2025-06-03 11:01:36,323:INFO:Gradient Boosting Classifier Imported successfully
2025-06-03 11:01:36,358:INFO:Starting cross validation
2025-06-03 11:01:36,375:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:01:55,535:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:55,565:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:55,582:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:01:55,832:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:09,373:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:09,500:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:09,585:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:09,617:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:19,677:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:19,752:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:19,782:INFO:Calculating mean and std
2025-06-03 11:02:19,809:INFO:Creating metrics dataframe
2025-06-03 11:02:19,817:INFO:Uploading results into container
2025-06-03 11:02:19,827:INFO:Uploading model into container now
2025-06-03 11:02:19,829:INFO:_master_model_container: 10
2025-06-03 11:02:19,829:INFO:_display_container: 2
2025-06-03 11:02:19,831:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8280, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-03 11:02:19,831:INFO:create_model() successfully completed......................................
2025-06-03 11:02:20,296:INFO:SubProcess create_model() end ==================================
2025-06-03 11:02:20,296:INFO:Creating metrics dataframe
2025-06-03 11:02:20,346:INFO:Initializing Linear Discriminant Analysis
2025-06-03 11:02:20,346:INFO:Total runtime is 1.525005845228831 minutes
2025-06-03 11:02:20,365:INFO:SubProcess create_model() called ==================================
2025-06-03 11:02:20,367:INFO:Initializing create_model()
2025-06-03 11:02:20,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:02:20,367:INFO:Checking exceptions
2025-06-03 11:02:20,367:INFO:Importing libraries
2025-06-03 11:02:20,367:INFO:Copying training dataset
2025-06-03 11:02:20,513:INFO:Defining folds
2025-06-03 11:02:20,513:INFO:Declaring metric variables
2025-06-03 11:02:20,540:INFO:Importing untrained model
2025-06-03 11:02:20,563:INFO:Linear Discriminant Analysis Imported successfully
2025-06-03 11:02:20,604:INFO:Starting cross validation
2025-06-03 11:02:20,608:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:02:20,802:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:20,810:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:20,826:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:20,828:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,100:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,161:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,179:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,181:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,329:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,374:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:02:21,512:INFO:Calculating mean and std
2025-06-03 11:02:21,516:INFO:Creating metrics dataframe
2025-06-03 11:02:21,527:INFO:Uploading results into container
2025-06-03 11:02:21,529:INFO:Uploading model into container now
2025-06-03 11:02:21,529:INFO:_master_model_container: 11
2025-06-03 11:02:21,529:INFO:_display_container: 2
2025-06-03 11:02:21,531:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-03 11:02:21,533:INFO:create_model() successfully completed......................................
2025-06-03 11:02:21,742:INFO:SubProcess create_model() end ==================================
2025-06-03 11:02:21,742:INFO:Creating metrics dataframe
2025-06-03 11:02:21,764:INFO:Initializing Extra Trees Classifier
2025-06-03 11:02:21,764:INFO:Total runtime is 1.548651393254598 minutes
2025-06-03 11:02:21,780:INFO:SubProcess create_model() called ==================================
2025-06-03 11:02:21,780:INFO:Initializing create_model()
2025-06-03 11:02:21,780:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:02:21,780:INFO:Checking exceptions
2025-06-03 11:02:21,788:INFO:Importing libraries
2025-06-03 11:02:21,788:INFO:Copying training dataset
2025-06-03 11:02:21,936:INFO:Defining folds
2025-06-03 11:02:21,938:INFO:Declaring metric variables
2025-06-03 11:02:21,977:INFO:Importing untrained model
2025-06-03 11:02:22,002:INFO:Extra Trees Classifier Imported successfully
2025-06-03 11:02:22,043:INFO:Starting cross validation
2025-06-03 11:02:22,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:02:27,500:INFO:Calculating mean and std
2025-06-03 11:02:27,505:INFO:Creating metrics dataframe
2025-06-03 11:02:27,517:INFO:Uploading results into container
2025-06-03 11:02:27,521:INFO:Uploading model into container now
2025-06-03 11:02:27,526:INFO:_master_model_container: 12
2025-06-03 11:02:27,526:INFO:_display_container: 2
2025-06-03 11:02:27,531:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8280, verbose=0,
                     warm_start=False)
2025-06-03 11:02:27,533:INFO:create_model() successfully completed......................................
2025-06-03 11:02:27,743:INFO:SubProcess create_model() end ==================================
2025-06-03 11:02:27,743:INFO:Creating metrics dataframe
2025-06-03 11:02:27,802:INFO:Initializing Light Gradient Boosting Machine
2025-06-03 11:02:27,806:INFO:Total runtime is 1.6493524630864462 minutes
2025-06-03 11:02:27,828:INFO:SubProcess create_model() called ==================================
2025-06-03 11:02:27,829:INFO:Initializing create_model()
2025-06-03 11:02:27,829:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:02:27,829:INFO:Checking exceptions
2025-06-03 11:02:27,829:INFO:Importing libraries
2025-06-03 11:02:27,829:INFO:Copying training dataset
2025-06-03 11:02:27,862:INFO:Defining folds
2025-06-03 11:02:27,862:INFO:Declaring metric variables
2025-06-03 11:02:27,883:INFO:Importing untrained model
2025-06-03 11:02:27,910:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-03 11:02:27,972:INFO:Starting cross validation
2025-06-03 11:02:28,034:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:02:57,603:INFO:Calculating mean and std
2025-06-03 11:02:57,609:INFO:Creating metrics dataframe
2025-06-03 11:02:57,633:INFO:Uploading results into container
2025-06-03 11:02:57,637:INFO:Uploading model into container now
2025-06-03 11:02:57,639:INFO:_master_model_container: 13
2025-06-03 11:02:57,639:INFO:_display_container: 2
2025-06-03 11:02:57,641:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8280, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-03 11:02:57,641:INFO:create_model() successfully completed......................................
2025-06-03 11:02:57,829:INFO:SubProcess create_model() end ==================================
2025-06-03 11:02:57,829:INFO:Creating metrics dataframe
2025-06-03 11:02:57,857:INFO:Initializing Dummy Classifier
2025-06-03 11:02:57,857:INFO:Total runtime is 2.1501877029736836 minutes
2025-06-03 11:02:57,865:INFO:SubProcess create_model() called ==================================
2025-06-03 11:02:57,865:INFO:Initializing create_model()
2025-06-03 11:02:57,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4EC5B23B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:02:57,865:INFO:Checking exceptions
2025-06-03 11:02:57,865:INFO:Importing libraries
2025-06-03 11:02:57,865:INFO:Copying training dataset
2025-06-03 11:02:57,922:INFO:Defining folds
2025-06-03 11:02:57,925:INFO:Declaring metric variables
2025-06-03 11:02:57,957:INFO:Importing untrained model
2025-06-03 11:02:58,014:INFO:Dummy Classifier Imported successfully
2025-06-03 11:02:58,040:INFO:Starting cross validation
2025-06-03 11:02:58,047:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:02:58,284:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,286:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,303:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,305:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,449:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,459:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,469:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,491:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,577:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,590:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:02:58,618:INFO:Calculating mean and std
2025-06-03 11:02:58,622:INFO:Creating metrics dataframe
2025-06-03 11:02:58,630:INFO:Uploading results into container
2025-06-03 11:02:58,630:INFO:Uploading model into container now
2025-06-03 11:02:58,632:INFO:_master_model_container: 14
2025-06-03 11:02:58,632:INFO:_display_container: 2
2025-06-03 11:02:58,632:INFO:DummyClassifier(constant=None, random_state=8280, strategy='prior')
2025-06-03 11:02:58,635:INFO:create_model() successfully completed......................................
2025-06-03 11:02:58,811:INFO:SubProcess create_model() end ==================================
2025-06-03 11:02:58,815:INFO:Creating metrics dataframe
2025-06-03 11:02:58,872:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-03 11:02:58,921:INFO:Initializing create_model()
2025-06-03 11:02:58,923:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4E77C8C10>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:02:58,923:INFO:Checking exceptions
2025-06-03 11:02:58,934:INFO:Importing libraries
2025-06-03 11:02:58,934:INFO:Copying training dataset
2025-06-03 11:02:59,057:INFO:Defining folds
2025-06-03 11:02:59,059:INFO:Declaring metric variables
2025-06-03 11:02:59,059:INFO:Importing untrained model
2025-06-03 11:02:59,059:INFO:Declaring custom model
2025-06-03 11:02:59,065:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:02:59,075:INFO:Cross validation set to False
2025-06-03 11:02:59,075:INFO:Fitting Model
2025-06-03 11:02:59,213:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:02:59,219:INFO:create_model() successfully completed......................................
2025-06-03 11:02:59,693:INFO:_master_model_container: 14
2025-06-03 11:02:59,693:INFO:_display_container: 2
2025-06-03 11:02:59,697:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:02:59,697:INFO:compare_models() successfully completed......................................
2025-06-03 11:38:50,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:38:50,921:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:38:50,921:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:38:50,921:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:38:54,712:INFO:PyCaret ClassificationExperiment
2025-06-03 11:38:54,714:INFO:Logging name: clf-default-name
2025-06-03 11:38:54,720:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-03 11:38:54,722:INFO:version 3.3.2
2025-06-03 11:38:54,728:INFO:Initializing setup()
2025-06-03 11:38:54,730:INFO:self.USI: 8ca4
2025-06-03 11:38:54,737:INFO:self._variable_keys: {'target_param', 'idx', 'y_train', 'n_jobs_param', 'y_test', 'pipeline', 'logging_param', 'html_param', '_ml_usecase', 'gpu_n_jobs_param', 'X_test', 'exp_name_log', 'X', 'seed', 'X_train', 'data', 'fix_imbalance', 'is_multiclass', 'fold_groups_param', 'USI', 'memory', 'fold_shuffle_param', 'gpu_param', '_available_plots', 'fold_generator', 'y', 'exp_id', 'log_plots_param'}
2025-06-03 11:38:54,740:INFO:Checking environment
2025-06-03 11:38:54,740:INFO:python_version: 3.10.0
2025-06-03 11:38:54,744:INFO:python_build: ('tags/v3.10.0:b494f59', 'Oct  4 2021 19:00:18')
2025-06-03 11:38:54,744:INFO:machine: AMD64
2025-06-03 11:38:54,746:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-03 11:38:54,787:INFO:Memory: svmem(total=4110286848, available=640483328, percent=84.4, used=3469803520, free=640483328)
2025-06-03 11:38:54,791:INFO:Physical Core: 4
2025-06-03 11:38:54,793:INFO:Logical Core: 4
2025-06-03 11:38:54,793:INFO:Checking libraries
2025-06-03 11:38:54,793:INFO:System:
2025-06-03 11:38:54,795:INFO:    python: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
2025-06-03 11:38:54,795:INFO:executable: c:\Program Files\Python310\python.exe
2025-06-03 11:38:54,795:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-03 11:38:54,803:INFO:PyCaret required dependencies:
2025-06-03 11:38:55,296:INFO:                 pip: 21.2.3
2025-06-03 11:38:55,298:INFO:          setuptools: 57.4.0
2025-06-03 11:38:55,298:INFO:             pycaret: 3.3.2
2025-06-03 11:38:55,298:INFO:             IPython: 8.30.0
2025-06-03 11:38:55,298:INFO:          ipywidgets: 8.1.5
2025-06-03 11:38:55,298:INFO:                tqdm: 4.67.1
2025-06-03 11:38:55,298:INFO:               numpy: 1.26.4
2025-06-03 11:38:55,302:INFO:              pandas: 2.1.4
2025-06-03 11:38:55,304:INFO:              jinja2: 3.1.4
2025-06-03 11:38:55,304:INFO:               scipy: 1.11.4
2025-06-03 11:38:55,306:INFO:              joblib: 1.3.2
2025-06-03 11:38:55,306:INFO:             sklearn: 1.4.2
2025-06-03 11:38:55,306:INFO:                pyod: 2.0.5
2025-06-03 11:38:55,308:INFO:            imblearn: 0.13.0
2025-06-03 11:38:55,308:INFO:   category_encoders: 2.7.0
2025-06-03 11:38:55,308:INFO:            lightgbm: 4.6.0
2025-06-03 11:38:55,308:INFO:               numba: 0.61.2
2025-06-03 11:38:55,310:INFO:            requests: 2.32.3
2025-06-03 11:38:55,310:INFO:          matplotlib: 3.7.5
2025-06-03 11:38:55,310:INFO:          scikitplot: 0.3.7
2025-06-03 11:38:55,310:INFO:         yellowbrick: 1.5
2025-06-03 11:38:55,310:INFO:              plotly: 5.24.1
2025-06-03 11:38:55,312:INFO:    plotly-resampler: Not installed
2025-06-03 11:38:55,312:INFO:             kaleido: 0.2.1
2025-06-03 11:38:55,312:INFO:           schemdraw: 0.15
2025-06-03 11:38:55,312:INFO:         statsmodels: 0.14.4
2025-06-03 11:38:55,312:INFO:              sktime: 0.26.0
2025-06-03 11:38:55,314:INFO:               tbats: 1.1.3
2025-06-03 11:38:55,314:INFO:            pmdarima: 2.0.4
2025-06-03 11:38:55,314:INFO:              psutil: 6.1.0
2025-06-03 11:38:55,314:INFO:          markupsafe: 3.0.2
2025-06-03 11:38:55,314:INFO:             pickle5: Not installed
2025-06-03 11:38:55,314:INFO:         cloudpickle: 3.1.1
2025-06-03 11:38:55,314:INFO:         deprecation: 2.1.0
2025-06-03 11:38:55,314:INFO:              xxhash: 3.5.0
2025-06-03 11:38:55,314:INFO:           wurlitzer: Not installed
2025-06-03 11:38:55,314:INFO:PyCaret optional dependencies:
2025-06-03 11:38:55,369:INFO:                shap: Not installed
2025-06-03 11:38:55,371:INFO:           interpret: Not installed
2025-06-03 11:38:55,371:INFO:                umap: Not installed
2025-06-03 11:38:55,371:INFO:     ydata_profiling: Not installed
2025-06-03 11:38:55,371:INFO:  explainerdashboard: Not installed
2025-06-03 11:38:55,371:INFO:             autoviz: Not installed
2025-06-03 11:38:55,373:INFO:           fairlearn: Not installed
2025-06-03 11:38:55,373:INFO:          deepchecks: Not installed
2025-06-03 11:38:55,373:INFO:             xgboost: Not installed
2025-06-03 11:38:55,373:INFO:            catboost: Not installed
2025-06-03 11:38:55,373:INFO:              kmodes: Not installed
2025-06-03 11:38:55,373:INFO:             mlxtend: Not installed
2025-06-03 11:38:55,373:INFO:       statsforecast: Not installed
2025-06-03 11:38:55,373:INFO:        tune_sklearn: Not installed
2025-06-03 11:38:55,373:INFO:                 ray: Not installed
2025-06-03 11:38:55,373:INFO:            hyperopt: Not installed
2025-06-03 11:38:55,373:INFO:              optuna: Not installed
2025-06-03 11:38:55,373:INFO:               skopt: Not installed
2025-06-03 11:38:55,373:INFO:              mlflow: Not installed
2025-06-03 11:38:55,373:INFO:              gradio: Not installed
2025-06-03 11:38:55,373:INFO:             fastapi: Not installed
2025-06-03 11:38:55,373:INFO:             uvicorn: Not installed
2025-06-03 11:38:55,373:INFO:              m2cgen: Not installed
2025-06-03 11:38:55,373:INFO:           evidently: Not installed
2025-06-03 11:38:55,373:INFO:               fugue: Not installed
2025-06-03 11:38:55,373:INFO:           streamlit: 1.44.1
2025-06-03 11:38:55,373:INFO:             prophet: Not installed
2025-06-03 11:38:55,373:INFO:None
2025-06-03 11:38:55,373:INFO:Set up data.
2025-06-03 11:38:55,430:INFO:Set up folding strategy.
2025-06-03 11:38:55,430:INFO:Set up train/test split.
2025-06-03 11:38:55,470:INFO:Set up index.
2025-06-03 11:38:55,472:INFO:Assigning column types.
2025-06-03 11:38:55,508:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-03 11:38:55,728:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 11:38:55,731:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:38:55,965:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:55,968:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:56,367:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 11:38:56,367:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:38:56,521:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:56,521:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:56,527:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-03 11:38:56,820:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:38:56,955:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:56,955:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:57,145:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:38:57,376:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:57,376:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:57,378:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-03 11:38:57,809:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:57,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:58,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:58,352:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:58,396:INFO:Preparing preprocessing pipeline...
2025-06-03 11:38:58,405:INFO:Set up simple imputation.
2025-06-03 11:38:58,676:INFO:Finished creating preprocessing pipeline.
2025-06-03 11:38:58,719:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\NEVILL~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['pixel_0_0', 'pixel_0_1',
                                             'pixel_0_2', 'pixel_0_3',
                                             'pixel_0_4', 'pixel_0_5',
                                             'pixel_0_6', 'pixel_0_7',
                                             'pixel_1_0', 'pixel_1_1',
                                             'pixel_1_2', 'pixel_1_3',
                                             'pixel_1_4', 'pixel_1_5',
                                             'pixel_1_6', 'pixel_1_7',
                                             'pixel_2...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-06-03 11:38:58,719:INFO:Creating final display dataframe.
2025-06-03 11:38:59,366:INFO:Setup _display_container:                     Description             Value
0                    Session id              6904
1                        Target            target
2                   Target type        Multiclass
3           Original data shape        (1797, 65)
4        Transformed data shape        (1797, 65)
5   Transformed train set shape        (1257, 65)
6    Transformed test set shape         (540, 65)
7              Numeric features                64
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              8ca4
2025-06-03 11:38:59,741:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:38:59,742:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:39:00,273:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:39:00,273:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:39:00,281:INFO:setup() successfully completed in 5.59s...............
2025-06-03 11:39:00,345:INFO:Initializing compare_models()
2025-06-03 11:39:00,345:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-03 11:39:00,345:INFO:Checking exceptions
2025-06-03 11:39:00,582:INFO:Preparing display monitor
2025-06-03 11:39:00,865:INFO:Initializing Logistic Regression
2025-06-03 11:39:00,867:INFO:Total runtime is 0.00016384522120157878 minutes
2025-06-03 11:39:00,911:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:00,915:INFO:Initializing create_model()
2025-06-03 11:39:00,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:00,915:INFO:Checking exceptions
2025-06-03 11:39:00,915:INFO:Importing libraries
2025-06-03 11:39:00,915:INFO:Copying training dataset
2025-06-03 11:39:01,014:INFO:Defining folds
2025-06-03 11:39:01,014:INFO:Declaring metric variables
2025-06-03 11:39:01,041:INFO:Importing untrained model
2025-06-03 11:39:01,079:INFO:Logistic Regression Imported successfully
2025-06-03 11:39:01,111:INFO:Starting cross validation
2025-06-03 11:39:01,117:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:22,481:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:23,412:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:23,436:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:23,464:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:23,515:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:24,002:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:24,015:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:24,085:INFO:Calculating mean and std
2025-06-03 11:39:24,124:INFO:Creating metrics dataframe
2025-06-03 11:39:24,158:INFO:Uploading results into container
2025-06-03 11:39:24,158:INFO:Uploading model into container now
2025-06-03 11:39:24,178:INFO:_master_model_container: 1
2025-06-03 11:39:24,178:INFO:_display_container: 2
2025-06-03 11:39:24,179:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6904, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-03 11:39:24,179:INFO:create_model() successfully completed......................................
2025-06-03 11:39:25,556:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:25,556:INFO:Creating metrics dataframe
2025-06-03 11:39:25,632:INFO:Initializing K Neighbors Classifier
2025-06-03 11:39:25,632:INFO:Total runtime is 0.4129181027412414 minutes
2025-06-03 11:39:25,640:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:25,640:INFO:Initializing create_model()
2025-06-03 11:39:25,651:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:25,651:INFO:Checking exceptions
2025-06-03 11:39:25,653:INFO:Importing libraries
2025-06-03 11:39:25,656:INFO:Copying training dataset
2025-06-03 11:39:25,826:INFO:Defining folds
2025-06-03 11:39:25,828:INFO:Declaring metric variables
2025-06-03 11:39:25,858:INFO:Importing untrained model
2025-06-03 11:39:25,870:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:39:25,904:INFO:Starting cross validation
2025-06-03 11:39:25,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:27,943:INFO:Calculating mean and std
2025-06-03 11:39:27,943:INFO:Creating metrics dataframe
2025-06-03 11:39:27,948:INFO:Uploading results into container
2025-06-03 11:39:27,948:INFO:Uploading model into container now
2025-06-03 11:39:27,948:INFO:_master_model_container: 2
2025-06-03 11:39:27,948:INFO:_display_container: 2
2025-06-03 11:39:27,948:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:39:27,948:INFO:create_model() successfully completed......................................
2025-06-03 11:39:28,132:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:28,132:INFO:Creating metrics dataframe
2025-06-03 11:39:28,177:INFO:Initializing Naive Bayes
2025-06-03 11:39:28,177:INFO:Total runtime is 0.4553273638089497 minutes
2025-06-03 11:39:28,185:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:28,185:INFO:Initializing create_model()
2025-06-03 11:39:28,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:28,189:INFO:Checking exceptions
2025-06-03 11:39:28,190:INFO:Importing libraries
2025-06-03 11:39:28,190:INFO:Copying training dataset
2025-06-03 11:39:28,231:INFO:Defining folds
2025-06-03 11:39:28,231:INFO:Declaring metric variables
2025-06-03 11:39:28,242:INFO:Importing untrained model
2025-06-03 11:39:28,251:INFO:Naive Bayes Imported successfully
2025-06-03 11:39:28,267:INFO:Starting cross validation
2025-06-03 11:39:28,271:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:28,843:INFO:Calculating mean and std
2025-06-03 11:39:28,847:INFO:Creating metrics dataframe
2025-06-03 11:39:28,849:INFO:Uploading results into container
2025-06-03 11:39:28,851:INFO:Uploading model into container now
2025-06-03 11:39:28,853:INFO:_master_model_container: 3
2025-06-03 11:39:28,853:INFO:_display_container: 2
2025-06-03 11:39:28,854:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-03 11:39:28,854:INFO:create_model() successfully completed......................................
2025-06-03 11:39:29,043:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:29,045:INFO:Creating metrics dataframe
2025-06-03 11:39:29,113:INFO:Initializing Decision Tree Classifier
2025-06-03 11:39:29,113:INFO:Total runtime is 0.4709336360295613 minutes
2025-06-03 11:39:29,167:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:29,169:INFO:Initializing create_model()
2025-06-03 11:39:29,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:29,170:INFO:Checking exceptions
2025-06-03 11:39:29,170:INFO:Importing libraries
2025-06-03 11:39:29,170:INFO:Copying training dataset
2025-06-03 11:39:29,780:INFO:Defining folds
2025-06-03 11:39:29,780:INFO:Declaring metric variables
2025-06-03 11:39:29,793:INFO:Importing untrained model
2025-06-03 11:39:29,810:INFO:Decision Tree Classifier Imported successfully
2025-06-03 11:39:29,848:INFO:Starting cross validation
2025-06-03 11:39:29,851:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:31,428:INFO:Calculating mean and std
2025-06-03 11:39:31,475:INFO:Creating metrics dataframe
2025-06-03 11:39:31,483:INFO:Uploading results into container
2025-06-03 11:39:31,485:INFO:Uploading model into container now
2025-06-03 11:39:31,487:INFO:_master_model_container: 4
2025-06-03 11:39:31,487:INFO:_display_container: 2
2025-06-03 11:39:31,489:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6904, splitter='best')
2025-06-03 11:39:31,489:INFO:create_model() successfully completed......................................
2025-06-03 11:39:32,095:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:32,095:INFO:Creating metrics dataframe
2025-06-03 11:39:32,156:INFO:Initializing SVM - Linear Kernel
2025-06-03 11:39:32,158:INFO:Total runtime is 0.5216758926709493 minutes
2025-06-03 11:39:32,175:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:32,178:INFO:Initializing create_model()
2025-06-03 11:39:32,180:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:32,180:INFO:Checking exceptions
2025-06-03 11:39:32,180:INFO:Importing libraries
2025-06-03 11:39:32,180:INFO:Copying training dataset
2025-06-03 11:39:32,324:INFO:Defining folds
2025-06-03 11:39:32,327:INFO:Declaring metric variables
2025-06-03 11:39:32,346:INFO:Importing untrained model
2025-06-03 11:39:32,370:INFO:SVM - Linear Kernel Imported successfully
2025-06-03 11:39:32,438:INFO:Starting cross validation
2025-06-03 11:39:32,444:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:32,857:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:32,870:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:32,872:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:32,886:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,251:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,261:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,263:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,322:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,800:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,905:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:33,961:INFO:Calculating mean and std
2025-06-03 11:39:33,965:INFO:Creating metrics dataframe
2025-06-03 11:39:33,976:INFO:Uploading results into container
2025-06-03 11:39:33,978:INFO:Uploading model into container now
2025-06-03 11:39:33,980:INFO:_master_model_container: 5
2025-06-03 11:39:33,980:INFO:_display_container: 2
2025-06-03 11:39:33,982:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6904, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-03 11:39:33,984:INFO:create_model() successfully completed......................................
2025-06-03 11:39:34,288:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:34,308:INFO:Creating metrics dataframe
2025-06-03 11:39:34,389:INFO:Initializing Ridge Classifier
2025-06-03 11:39:34,389:INFO:Total runtime is 0.5588734149932861 minutes
2025-06-03 11:39:34,442:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:34,442:INFO:Initializing create_model()
2025-06-03 11:39:34,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:34,442:INFO:Checking exceptions
2025-06-03 11:39:34,442:INFO:Importing libraries
2025-06-03 11:39:34,444:INFO:Copying training dataset
2025-06-03 11:39:34,672:INFO:Defining folds
2025-06-03 11:39:34,672:INFO:Declaring metric variables
2025-06-03 11:39:34,683:INFO:Importing untrained model
2025-06-03 11:39:34,693:INFO:Ridge Classifier Imported successfully
2025-06-03 11:39:34,806:INFO:Starting cross validation
2025-06-03 11:39:34,810:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:35,046:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,064:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,121:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,141:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,529:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,600:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,606:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,612:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,779:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,806:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:35,878:INFO:Calculating mean and std
2025-06-03 11:39:35,888:INFO:Creating metrics dataframe
2025-06-03 11:39:35,903:INFO:Uploading results into container
2025-06-03 11:39:35,905:INFO:Uploading model into container now
2025-06-03 11:39:35,907:INFO:_master_model_container: 6
2025-06-03 11:39:35,907:INFO:_display_container: 2
2025-06-03 11:39:35,909:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6904, solver='auto',
                tol=0.0001)
2025-06-03 11:39:35,910:INFO:create_model() successfully completed......................................
2025-06-03 11:39:36,341:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:36,343:INFO:Creating metrics dataframe
2025-06-03 11:39:36,412:INFO:Initializing Random Forest Classifier
2025-06-03 11:39:36,412:INFO:Total runtime is 0.5925896326700847 minutes
2025-06-03 11:39:36,431:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:36,433:INFO:Initializing create_model()
2025-06-03 11:39:36,433:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:36,436:INFO:Checking exceptions
2025-06-03 11:39:36,438:INFO:Importing libraries
2025-06-03 11:39:36,439:INFO:Copying training dataset
2025-06-03 11:39:36,673:INFO:Defining folds
2025-06-03 11:39:36,675:INFO:Declaring metric variables
2025-06-03 11:39:36,702:INFO:Importing untrained model
2025-06-03 11:39:36,718:INFO:Random Forest Classifier Imported successfully
2025-06-03 11:39:36,759:INFO:Starting cross validation
2025-06-03 11:39:36,761:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:44,454:INFO:Calculating mean and std
2025-06-03 11:39:44,460:INFO:Creating metrics dataframe
2025-06-03 11:39:44,473:INFO:Uploading results into container
2025-06-03 11:39:44,475:INFO:Uploading model into container now
2025-06-03 11:39:44,480:INFO:_master_model_container: 7
2025-06-03 11:39:44,480:INFO:_display_container: 2
2025-06-03 11:39:44,482:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6904, verbose=0,
                       warm_start=False)
2025-06-03 11:39:44,482:INFO:create_model() successfully completed......................................
2025-06-03 11:39:44,737:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:44,737:INFO:Creating metrics dataframe
2025-06-03 11:39:44,777:INFO:Initializing Quadratic Discriminant Analysis
2025-06-03 11:39:44,777:INFO:Total runtime is 0.7319915533065796 minutes
2025-06-03 11:39:44,794:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:44,799:INFO:Initializing create_model()
2025-06-03 11:39:44,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:44,804:INFO:Checking exceptions
2025-06-03 11:39:44,806:INFO:Importing libraries
2025-06-03 11:39:44,806:INFO:Copying training dataset
2025-06-03 11:39:44,905:INFO:Defining folds
2025-06-03 11:39:44,905:INFO:Declaring metric variables
2025-06-03 11:39:44,927:INFO:Importing untrained model
2025-06-03 11:39:44,946:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-03 11:39:44,983:INFO:Starting cross validation
2025-06-03 11:39:44,987:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:45,198:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,202:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,214:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,230:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,278:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,280:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,299:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,310:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,363:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,367:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,398:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,447:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,491:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,547:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,573:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,595:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,613:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,653:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,666:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,685:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,693:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,700:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,709:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,757:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,773:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:39:45,834:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,843:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:45,848:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,857:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:45,882:INFO:Calculating mean and std
2025-06-03 11:39:45,885:INFO:Creating metrics dataframe
2025-06-03 11:39:45,900:INFO:Uploading results into container
2025-06-03 11:39:45,902:INFO:Uploading model into container now
2025-06-03 11:39:45,902:INFO:_master_model_container: 8
2025-06-03 11:39:45,902:INFO:_display_container: 2
2025-06-03 11:39:45,904:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-03 11:39:45,904:INFO:create_model() successfully completed......................................
2025-06-03 11:39:46,119:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:46,119:INFO:Creating metrics dataframe
2025-06-03 11:39:46,154:INFO:Initializing Ada Boost Classifier
2025-06-03 11:39:46,156:INFO:Total runtime is 0.7549824992815654 minutes
2025-06-03 11:39:46,170:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:46,170:INFO:Initializing create_model()
2025-06-03 11:39:46,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:46,170:INFO:Checking exceptions
2025-06-03 11:39:46,170:INFO:Importing libraries
2025-06-03 11:39:46,172:INFO:Copying training dataset
2025-06-03 11:39:46,379:INFO:Defining folds
2025-06-03 11:39:46,379:INFO:Declaring metric variables
2025-06-03 11:39:46,395:INFO:Importing untrained model
2025-06-03 11:39:46,407:INFO:Ada Boost Classifier Imported successfully
2025-06-03 11:39:46,447:INFO:Starting cross validation
2025-06-03 11:39:46,453:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:39:46,712:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:46,752:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:46,758:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:46,784:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:47,487:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:47,494:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:47,503:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:47,509:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:47,574:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:47,589:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:47,595:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:47,603:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:47,609:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:47,609:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:47,681:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:47,689:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:48,260:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:48,274:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:48,278:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:48,286:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:48,338:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:48,354:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:48,356:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:48,362:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:39:48,376:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:48,390:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:49,033:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:49,043:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:39:49,049:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:49,056:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:39:49,079:INFO:Calculating mean and std
2025-06-03 11:39:49,083:INFO:Creating metrics dataframe
2025-06-03 11:39:49,091:INFO:Uploading results into container
2025-06-03 11:39:49,095:INFO:Uploading model into container now
2025-06-03 11:39:49,097:INFO:_master_model_container: 9
2025-06-03 11:39:49,099:INFO:_display_container: 2
2025-06-03 11:39:49,100:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6904)
2025-06-03 11:39:49,102:INFO:create_model() successfully completed......................................
2025-06-03 11:39:49,333:INFO:SubProcess create_model() end ==================================
2025-06-03 11:39:49,333:INFO:Creating metrics dataframe
2025-06-03 11:39:49,367:INFO:Initializing Gradient Boosting Classifier
2025-06-03 11:39:49,369:INFO:Total runtime is 0.8085249503453573 minutes
2025-06-03 11:39:49,384:INFO:SubProcess create_model() called ==================================
2025-06-03 11:39:49,386:INFO:Initializing create_model()
2025-06-03 11:39:49,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:39:49,386:INFO:Checking exceptions
2025-06-03 11:39:49,386:INFO:Importing libraries
2025-06-03 11:39:49,387:INFO:Copying training dataset
2025-06-03 11:39:49,478:INFO:Defining folds
2025-06-03 11:39:49,479:INFO:Declaring metric variables
2025-06-03 11:39:49,499:INFO:Importing untrained model
2025-06-03 11:39:49,512:INFO:Gradient Boosting Classifier Imported successfully
2025-06-03 11:39:49,590:INFO:Starting cross validation
2025-06-03 11:39:49,594:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:40:05,110:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:05,146:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:05,218:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:05,230:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:18,761:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:18,834:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:18,863:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:18,893:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:33,396:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:33,476:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:33,503:INFO:Calculating mean and std
2025-06-03 11:40:33,503:INFO:Creating metrics dataframe
2025-06-03 11:40:33,514:INFO:Uploading results into container
2025-06-03 11:40:33,517:INFO:Uploading model into container now
2025-06-03 11:40:33,520:INFO:_master_model_container: 10
2025-06-03 11:40:33,520:INFO:_display_container: 2
2025-06-03 11:40:33,524:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6904, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-03 11:40:33,524:INFO:create_model() successfully completed......................................
2025-06-03 11:40:33,698:INFO:SubProcess create_model() end ==================================
2025-06-03 11:40:33,698:INFO:Creating metrics dataframe
2025-06-03 11:40:33,726:INFO:Initializing Linear Discriminant Analysis
2025-06-03 11:40:33,726:INFO:Total runtime is 1.5478155573209127 minutes
2025-06-03 11:40:33,739:INFO:SubProcess create_model() called ==================================
2025-06-03 11:40:33,741:INFO:Initializing create_model()
2025-06-03 11:40:33,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:40:33,741:INFO:Checking exceptions
2025-06-03 11:40:33,741:INFO:Importing libraries
2025-06-03 11:40:33,741:INFO:Copying training dataset
2025-06-03 11:40:33,801:INFO:Defining folds
2025-06-03 11:40:33,804:INFO:Declaring metric variables
2025-06-03 11:40:33,829:INFO:Importing untrained model
2025-06-03 11:40:33,843:INFO:Linear Discriminant Analysis Imported successfully
2025-06-03 11:40:33,890:INFO:Starting cross validation
2025-06-03 11:40:33,894:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:40:34,056:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,060:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,068:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,088:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,212:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,220:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,226:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,253:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,352:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,354:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:40:34,392:INFO:Calculating mean and std
2025-06-03 11:40:34,396:INFO:Creating metrics dataframe
2025-06-03 11:40:34,408:INFO:Uploading results into container
2025-06-03 11:40:34,411:INFO:Uploading model into container now
2025-06-03 11:40:34,414:INFO:_master_model_container: 11
2025-06-03 11:40:34,414:INFO:_display_container: 2
2025-06-03 11:40:34,414:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-03 11:40:34,414:INFO:create_model() successfully completed......................................
2025-06-03 11:40:34,580:INFO:SubProcess create_model() end ==================================
2025-06-03 11:40:34,580:INFO:Creating metrics dataframe
2025-06-03 11:40:34,621:INFO:Initializing Extra Trees Classifier
2025-06-03 11:40:34,621:INFO:Total runtime is 1.562736713886261 minutes
2025-06-03 11:40:34,631:INFO:SubProcess create_model() called ==================================
2025-06-03 11:40:34,631:INFO:Initializing create_model()
2025-06-03 11:40:34,633:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:40:34,635:INFO:Checking exceptions
2025-06-03 11:40:34,635:INFO:Importing libraries
2025-06-03 11:40:34,635:INFO:Copying training dataset
2025-06-03 11:40:34,673:INFO:Defining folds
2025-06-03 11:40:34,673:INFO:Declaring metric variables
2025-06-03 11:40:34,676:INFO:Importing untrained model
2025-06-03 11:40:34,695:INFO:Extra Trees Classifier Imported successfully
2025-06-03 11:40:34,709:INFO:Starting cross validation
2025-06-03 11:40:34,709:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:40:38,201:INFO:Calculating mean and std
2025-06-03 11:40:38,205:INFO:Creating metrics dataframe
2025-06-03 11:40:38,216:INFO:Uploading results into container
2025-06-03 11:40:38,221:INFO:Uploading model into container now
2025-06-03 11:40:38,221:INFO:_master_model_container: 12
2025-06-03 11:40:38,223:INFO:_display_container: 2
2025-06-03 11:40:38,225:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6904, verbose=0,
                     warm_start=False)
2025-06-03 11:40:38,225:INFO:create_model() successfully completed......................................
2025-06-03 11:40:38,412:INFO:SubProcess create_model() end ==================================
2025-06-03 11:40:38,412:INFO:Creating metrics dataframe
2025-06-03 11:40:38,446:INFO:Initializing Light Gradient Boosting Machine
2025-06-03 11:40:38,446:INFO:Total runtime is 1.6264814178148905 minutes
2025-06-03 11:40:38,464:INFO:SubProcess create_model() called ==================================
2025-06-03 11:40:38,466:INFO:Initializing create_model()
2025-06-03 11:40:38,466:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:40:38,466:INFO:Checking exceptions
2025-06-03 11:40:38,466:INFO:Importing libraries
2025-06-03 11:40:38,466:INFO:Copying training dataset
2025-06-03 11:40:38,576:INFO:Defining folds
2025-06-03 11:40:38,578:INFO:Declaring metric variables
2025-06-03 11:40:38,601:INFO:Importing untrained model
2025-06-03 11:40:38,621:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-03 11:40:38,658:INFO:Starting cross validation
2025-06-03 11:40:38,662:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:41:01,530:INFO:Calculating mean and std
2025-06-03 11:41:01,534:INFO:Creating metrics dataframe
2025-06-03 11:41:01,541:INFO:Uploading results into container
2025-06-03 11:41:01,543:INFO:Uploading model into container now
2025-06-03 11:41:01,545:INFO:_master_model_container: 13
2025-06-03 11:41:01,545:INFO:_display_container: 2
2025-06-03 11:41:01,552:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6904, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-03 11:41:01,552:INFO:create_model() successfully completed......................................
2025-06-03 11:41:01,775:INFO:SubProcess create_model() end ==================================
2025-06-03 11:41:01,775:INFO:Creating metrics dataframe
2025-06-03 11:41:01,817:INFO:Initializing Dummy Classifier
2025-06-03 11:41:01,817:INFO:Total runtime is 2.015992740790049 minutes
2025-06-03 11:41:01,824:INFO:SubProcess create_model() called ==================================
2025-06-03 11:41:01,831:INFO:Initializing create_model()
2025-06-03 11:41:01,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024917DCDDB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:41:01,831:INFO:Checking exceptions
2025-06-03 11:41:01,831:INFO:Importing libraries
2025-06-03 11:41:01,831:INFO:Copying training dataset
2025-06-03 11:41:01,880:INFO:Defining folds
2025-06-03 11:41:01,880:INFO:Declaring metric variables
2025-06-03 11:41:01,907:INFO:Importing untrained model
2025-06-03 11:41:01,927:INFO:Dummy Classifier Imported successfully
2025-06-03 11:41:01,956:INFO:Starting cross validation
2025-06-03 11:41:01,956:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:41:02,115:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,117:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,119:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,127:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,260:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,270:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,283:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,293:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,376:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,386:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:41:02,410:INFO:Calculating mean and std
2025-06-03 11:41:02,415:INFO:Creating metrics dataframe
2025-06-03 11:41:02,428:INFO:Uploading results into container
2025-06-03 11:41:02,430:INFO:Uploading model into container now
2025-06-03 11:41:02,432:INFO:_master_model_container: 14
2025-06-03 11:41:02,432:INFO:_display_container: 2
2025-06-03 11:41:02,432:INFO:DummyClassifier(constant=None, random_state=6904, strategy='prior')
2025-06-03 11:41:02,432:INFO:create_model() successfully completed......................................
2025-06-03 11:41:02,611:INFO:SubProcess create_model() end ==================================
2025-06-03 11:41:02,611:INFO:Creating metrics dataframe
2025-06-03 11:41:02,804:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-03 11:41:02,842:INFO:Initializing create_model()
2025-06-03 11:41:02,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024916934820>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:41:02,843:INFO:Checking exceptions
2025-06-03 11:41:02,857:INFO:Importing libraries
2025-06-03 11:41:02,857:INFO:Copying training dataset
2025-06-03 11:41:02,907:INFO:Defining folds
2025-06-03 11:41:02,907:INFO:Declaring metric variables
2025-06-03 11:41:02,907:INFO:Importing untrained model
2025-06-03 11:41:02,907:INFO:Declaring custom model
2025-06-03 11:41:02,907:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:41:02,916:INFO:Cross validation set to False
2025-06-03 11:41:02,916:INFO:Fitting Model
2025-06-03 11:41:02,952:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:41:02,952:INFO:create_model() successfully completed......................................
2025-06-03 11:41:03,179:INFO:_master_model_container: 14
2025-06-03 11:41:03,179:INFO:_display_container: 2
2025-06-03 11:41:03,179:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:41:03,179:INFO:compare_models() successfully completed......................................
2025-06-03 11:52:03,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:52:03,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:52:03,118:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:52:03,119:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 11:52:05,373:INFO:PyCaret ClassificationExperiment
2025-06-03 11:52:05,379:INFO:Logging name: clf-default-name
2025-06-03 11:52:05,386:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-03 11:52:05,392:INFO:version 3.3.2
2025-06-03 11:52:05,394:INFO:Initializing setup()
2025-06-03 11:52:05,394:INFO:self.USI: da43
2025-06-03 11:52:05,400:INFO:self._variable_keys: {'y_test', 'X_test', 'is_multiclass', 'exp_id', 'y_train', 'y', '_available_plots', '_ml_usecase', 'target_param', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'html_param', 'fold_generator', 'gpu_n_jobs_param', 'n_jobs_param', 'exp_name_log', 'gpu_param', 'logging_param', 'data', 'X', 'USI', 'X_train', 'memory', 'pipeline', 'fix_imbalance', 'seed', 'idx'}
2025-06-03 11:52:05,404:INFO:Checking environment
2025-06-03 11:52:05,410:INFO:python_version: 3.10.0
2025-06-03 11:52:05,413:INFO:python_build: ('tags/v3.10.0:b494f59', 'Oct  4 2021 19:00:18')
2025-06-03 11:52:05,415:INFO:machine: AMD64
2025-06-03 11:52:05,415:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-03 11:52:05,449:INFO:Memory: svmem(total=4110286848, available=432971776, percent=89.5, used=3677315072, free=432971776)
2025-06-03 11:52:05,453:INFO:Physical Core: 4
2025-06-03 11:52:05,453:INFO:Logical Core: 4
2025-06-03 11:52:05,453:INFO:Checking libraries
2025-06-03 11:52:05,453:INFO:System:
2025-06-03 11:52:05,455:INFO:    python: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
2025-06-03 11:52:05,457:INFO:executable: c:\Program Files\Python310\python.exe
2025-06-03 11:52:05,457:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-03 11:52:05,457:INFO:PyCaret required dependencies:
2025-06-03 11:52:06,711:INFO:                 pip: 21.2.3
2025-06-03 11:52:06,711:INFO:          setuptools: 57.4.0
2025-06-03 11:52:06,711:INFO:             pycaret: 3.3.2
2025-06-03 11:52:06,711:INFO:             IPython: 8.30.0
2025-06-03 11:52:06,711:INFO:          ipywidgets: 8.1.5
2025-06-03 11:52:06,713:INFO:                tqdm: 4.67.1
2025-06-03 11:52:06,713:INFO:               numpy: 1.26.4
2025-06-03 11:52:06,715:INFO:              pandas: 2.1.4
2025-06-03 11:52:06,715:INFO:              jinja2: 3.1.4
2025-06-03 11:52:06,715:INFO:               scipy: 1.11.4
2025-06-03 11:52:06,717:INFO:              joblib: 1.3.2
2025-06-03 11:52:06,717:INFO:             sklearn: 1.4.2
2025-06-03 11:52:06,717:INFO:                pyod: 2.0.5
2025-06-03 11:52:06,719:INFO:            imblearn: 0.13.0
2025-06-03 11:52:06,719:INFO:   category_encoders: 2.7.0
2025-06-03 11:52:06,719:INFO:            lightgbm: 4.6.0
2025-06-03 11:52:06,719:INFO:               numba: 0.61.2
2025-06-03 11:52:06,719:INFO:            requests: 2.32.3
2025-06-03 11:52:06,721:INFO:          matplotlib: 3.7.5
2025-06-03 11:52:06,721:INFO:          scikitplot: 0.3.7
2025-06-03 11:52:06,721:INFO:         yellowbrick: 1.5
2025-06-03 11:52:06,721:INFO:              plotly: 5.24.1
2025-06-03 11:52:06,723:INFO:    plotly-resampler: Not installed
2025-06-03 11:52:06,723:INFO:             kaleido: 0.2.1
2025-06-03 11:52:06,723:INFO:           schemdraw: 0.15
2025-06-03 11:52:06,723:INFO:         statsmodels: 0.14.4
2025-06-03 11:52:06,723:INFO:              sktime: 0.26.0
2025-06-03 11:52:06,723:INFO:               tbats: 1.1.3
2025-06-03 11:52:06,725:INFO:            pmdarima: 2.0.4
2025-06-03 11:52:06,725:INFO:              psutil: 6.1.0
2025-06-03 11:52:06,725:INFO:          markupsafe: 3.0.2
2025-06-03 11:52:06,725:INFO:             pickle5: Not installed
2025-06-03 11:52:06,725:INFO:         cloudpickle: 3.1.1
2025-06-03 11:52:06,725:INFO:         deprecation: 2.1.0
2025-06-03 11:52:06,725:INFO:              xxhash: 3.5.0
2025-06-03 11:52:06,725:INFO:           wurlitzer: Not installed
2025-06-03 11:52:06,725:INFO:PyCaret optional dependencies:
2025-06-03 11:52:06,787:INFO:                shap: Not installed
2025-06-03 11:52:06,787:INFO:           interpret: Not installed
2025-06-03 11:52:06,787:INFO:                umap: Not installed
2025-06-03 11:52:06,789:INFO:     ydata_profiling: Not installed
2025-06-03 11:52:06,789:INFO:  explainerdashboard: Not installed
2025-06-03 11:52:06,789:INFO:             autoviz: Not installed
2025-06-03 11:52:06,789:INFO:           fairlearn: Not installed
2025-06-03 11:52:06,789:INFO:          deepchecks: Not installed
2025-06-03 11:52:06,789:INFO:             xgboost: Not installed
2025-06-03 11:52:06,789:INFO:            catboost: Not installed
2025-06-03 11:52:06,789:INFO:              kmodes: Not installed
2025-06-03 11:52:06,789:INFO:             mlxtend: Not installed
2025-06-03 11:52:06,789:INFO:       statsforecast: Not installed
2025-06-03 11:52:06,789:INFO:        tune_sklearn: Not installed
2025-06-03 11:52:06,789:INFO:                 ray: Not installed
2025-06-03 11:52:06,791:INFO:            hyperopt: Not installed
2025-06-03 11:52:06,791:INFO:              optuna: Not installed
2025-06-03 11:52:06,791:INFO:               skopt: Not installed
2025-06-03 11:52:06,791:INFO:              mlflow: Not installed
2025-06-03 11:52:06,791:INFO:              gradio: Not installed
2025-06-03 11:52:06,791:INFO:             fastapi: Not installed
2025-06-03 11:52:06,791:INFO:             uvicorn: Not installed
2025-06-03 11:52:06,791:INFO:              m2cgen: Not installed
2025-06-03 11:52:06,791:INFO:           evidently: Not installed
2025-06-03 11:52:06,791:INFO:               fugue: Not installed
2025-06-03 11:52:06,791:INFO:           streamlit: 1.44.1
2025-06-03 11:52:06,791:INFO:             prophet: Not installed
2025-06-03 11:52:06,791:INFO:None
2025-06-03 11:52:06,791:INFO:Set up data.
2025-06-03 11:52:06,874:INFO:Set up folding strategy.
2025-06-03 11:52:06,874:INFO:Set up train/test split.
2025-06-03 11:52:07,126:INFO:Set up index.
2025-06-03 11:52:07,142:INFO:Assigning column types.
2025-06-03 11:52:07,306:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-03 11:52:07,547:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 11:52:07,549:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:52:07,902:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:07,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,186:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 11:52:08,194:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:52:08,313:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,313:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,313:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-03 11:52:08,514:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:52:08,651:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,845:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 11:52:08,966:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:08,966:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-03 11:52:09,393:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:09,393:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:09,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:09,777:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:09,785:INFO:Preparing preprocessing pipeline...
2025-06-03 11:52:09,792:INFO:Set up simple imputation.
2025-06-03 11:52:10,083:INFO:Finished creating preprocessing pipeline.
2025-06-03 11:52:10,101:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\NEVILL~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['pixel_0_0', 'pixel_0_1',
                                             'pixel_0_2', 'pixel_0_3',
                                             'pixel_0_4', 'pixel_0_5',
                                             'pixel_0_6', 'pixel_0_7',
                                             'pixel_1_0', 'pixel_1_1',
                                             'pixel_1_2', 'pixel_1_3',
                                             'pixel_1_4', 'pixel_1_5',
                                             'pixel_1_6', 'pixel_1_7',
                                             'pixel_2...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-06-03 11:52:10,101:INFO:Creating final display dataframe.
2025-06-03 11:52:10,736:INFO:Setup _display_container:                     Description             Value
0                    Session id              5773
1                        Target            target
2                   Target type        Multiclass
3           Original data shape        (1797, 65)
4        Transformed data shape        (1797, 65)
5   Transformed train set shape        (1257, 65)
6    Transformed test set shape         (540, 65)
7              Numeric features                64
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              da43
2025-06-03 11:52:11,247:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:11,247:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:11,634:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:11,636:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 11:52:11,638:INFO:setup() successfully completed in 6.3s...............
2025-06-03 11:52:11,679:INFO:Initializing compare_models()
2025-06-03 11:52:11,679:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-03 11:52:11,679:INFO:Checking exceptions
2025-06-03 11:52:11,760:INFO:Preparing display monitor
2025-06-03 11:52:11,951:INFO:Initializing Logistic Regression
2025-06-03 11:52:11,951:INFO:Total runtime is 0.0 minutes
2025-06-03 11:52:11,995:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:11,997:INFO:Initializing create_model()
2025-06-03 11:52:11,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:11,997:INFO:Checking exceptions
2025-06-03 11:52:12,003:INFO:Importing libraries
2025-06-03 11:52:12,003:INFO:Copying training dataset
2025-06-03 11:52:12,262:INFO:Defining folds
2025-06-03 11:52:12,264:INFO:Declaring metric variables
2025-06-03 11:52:12,279:INFO:Importing untrained model
2025-06-03 11:52:12,298:INFO:Logistic Regression Imported successfully
2025-06-03 11:52:12,344:INFO:Starting cross validation
2025-06-03 11:52:12,344:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:29,474:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:29,496:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:29,498:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:29,508:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:30,379:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:30,420:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:30,438:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:30,446:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:30,941:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:31,024:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:31,065:INFO:Calculating mean and std
2025-06-03 11:52:31,068:INFO:Creating metrics dataframe
2025-06-03 11:52:31,078:INFO:Uploading results into container
2025-06-03 11:52:31,079:INFO:Uploading model into container now
2025-06-03 11:52:31,079:INFO:_master_model_container: 1
2025-06-03 11:52:31,079:INFO:_display_container: 2
2025-06-03 11:52:31,083:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5773, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-03 11:52:31,086:INFO:create_model() successfully completed......................................
2025-06-03 11:52:31,239:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:31,239:INFO:Creating metrics dataframe
2025-06-03 11:52:31,253:INFO:Initializing K Neighbors Classifier
2025-06-03 11:52:31,253:INFO:Total runtime is 0.32169058322906496 minutes
2025-06-03 11:52:31,261:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:31,268:INFO:Initializing create_model()
2025-06-03 11:52:31,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:31,268:INFO:Checking exceptions
2025-06-03 11:52:31,268:INFO:Importing libraries
2025-06-03 11:52:31,268:INFO:Copying training dataset
2025-06-03 11:52:31,312:INFO:Defining folds
2025-06-03 11:52:31,314:INFO:Declaring metric variables
2025-06-03 11:52:31,325:INFO:Importing untrained model
2025-06-03 11:52:31,336:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:52:31,358:INFO:Starting cross validation
2025-06-03 11:52:31,365:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:32,408:INFO:Calculating mean and std
2025-06-03 11:52:32,408:INFO:Creating metrics dataframe
2025-06-03 11:52:32,424:INFO:Uploading results into container
2025-06-03 11:52:32,425:INFO:Uploading model into container now
2025-06-03 11:52:32,425:INFO:_master_model_container: 2
2025-06-03 11:52:32,425:INFO:_display_container: 2
2025-06-03 11:52:32,425:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:52:32,425:INFO:create_model() successfully completed......................................
2025-06-03 11:52:32,586:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:32,586:INFO:Creating metrics dataframe
2025-06-03 11:52:32,596:INFO:Initializing Naive Bayes
2025-06-03 11:52:32,596:INFO:Total runtime is 0.34408871332804364 minutes
2025-06-03 11:52:32,612:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:32,612:INFO:Initializing create_model()
2025-06-03 11:52:32,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:32,612:INFO:Checking exceptions
2025-06-03 11:52:32,612:INFO:Importing libraries
2025-06-03 11:52:32,612:INFO:Copying training dataset
2025-06-03 11:52:32,659:INFO:Defining folds
2025-06-03 11:52:32,659:INFO:Declaring metric variables
2025-06-03 11:52:32,669:INFO:Importing untrained model
2025-06-03 11:52:32,677:INFO:Naive Bayes Imported successfully
2025-06-03 11:52:32,700:INFO:Starting cross validation
2025-06-03 11:52:32,706:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:33,276:INFO:Calculating mean and std
2025-06-03 11:52:33,276:INFO:Creating metrics dataframe
2025-06-03 11:52:33,276:INFO:Uploading results into container
2025-06-03 11:52:33,276:INFO:Uploading model into container now
2025-06-03 11:52:33,276:INFO:_master_model_container: 3
2025-06-03 11:52:33,276:INFO:_display_container: 2
2025-06-03 11:52:33,276:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-03 11:52:33,276:INFO:create_model() successfully completed......................................
2025-06-03 11:52:33,426:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:33,426:INFO:Creating metrics dataframe
2025-06-03 11:52:33,446:INFO:Initializing Decision Tree Classifier
2025-06-03 11:52:33,446:INFO:Total runtime is 0.3582423249880473 minutes
2025-06-03 11:52:33,452:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:33,460:INFO:Initializing create_model()
2025-06-03 11:52:33,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:33,460:INFO:Checking exceptions
2025-06-03 11:52:33,460:INFO:Importing libraries
2025-06-03 11:52:33,460:INFO:Copying training dataset
2025-06-03 11:52:33,502:INFO:Defining folds
2025-06-03 11:52:33,502:INFO:Declaring metric variables
2025-06-03 11:52:33,513:INFO:Importing untrained model
2025-06-03 11:52:33,526:INFO:Decision Tree Classifier Imported successfully
2025-06-03 11:52:33,544:INFO:Starting cross validation
2025-06-03 11:52:33,547:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:34,099:INFO:Calculating mean and std
2025-06-03 11:52:34,099:INFO:Creating metrics dataframe
2025-06-03 11:52:34,106:INFO:Uploading results into container
2025-06-03 11:52:34,106:INFO:Uploading model into container now
2025-06-03 11:52:34,106:INFO:_master_model_container: 4
2025-06-03 11:52:34,106:INFO:_display_container: 2
2025-06-03 11:52:34,106:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5773, splitter='best')
2025-06-03 11:52:34,106:INFO:create_model() successfully completed......................................
2025-06-03 11:52:34,259:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:34,259:INFO:Creating metrics dataframe
2025-06-03 11:52:34,275:INFO:Initializing SVM - Linear Kernel
2025-06-03 11:52:34,275:INFO:Total runtime is 0.37205618222554526 minutes
2025-06-03 11:52:34,275:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:34,290:INFO:Initializing create_model()
2025-06-03 11:52:34,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:34,290:INFO:Checking exceptions
2025-06-03 11:52:34,290:INFO:Importing libraries
2025-06-03 11:52:34,290:INFO:Copying training dataset
2025-06-03 11:52:34,327:INFO:Defining folds
2025-06-03 11:52:34,327:INFO:Declaring metric variables
2025-06-03 11:52:34,337:INFO:Importing untrained model
2025-06-03 11:52:34,337:INFO:SVM - Linear Kernel Imported successfully
2025-06-03 11:52:34,369:INFO:Starting cross validation
2025-06-03 11:52:34,369:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:34,563:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,601:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,783:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,806:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,808:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,824:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,946:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:34,960:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,002:INFO:Calculating mean and std
2025-06-03 11:52:35,002:INFO:Creating metrics dataframe
2025-06-03 11:52:35,013:INFO:Uploading results into container
2025-06-03 11:52:35,013:INFO:Uploading model into container now
2025-06-03 11:52:35,013:INFO:_master_model_container: 5
2025-06-03 11:52:35,013:INFO:_display_container: 2
2025-06-03 11:52:35,013:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5773, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-03 11:52:35,013:INFO:create_model() successfully completed......................................
2025-06-03 11:52:35,148:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:35,160:INFO:Creating metrics dataframe
2025-06-03 11:52:35,176:INFO:Initializing Ridge Classifier
2025-06-03 11:52:35,176:INFO:Total runtime is 0.38708120584487915 minutes
2025-06-03 11:52:35,189:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:35,189:INFO:Initializing create_model()
2025-06-03 11:52:35,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:35,189:INFO:Checking exceptions
2025-06-03 11:52:35,189:INFO:Importing libraries
2025-06-03 11:52:35,189:INFO:Copying training dataset
2025-06-03 11:52:35,228:INFO:Defining folds
2025-06-03 11:52:35,228:INFO:Declaring metric variables
2025-06-03 11:52:35,239:INFO:Importing untrained model
2025-06-03 11:52:35,239:INFO:Ridge Classifier Imported successfully
2025-06-03 11:52:35,257:INFO:Starting cross validation
2025-06-03 11:52:35,275:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:35,419:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,421:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,431:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,629:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,662:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,668:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,683:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,791:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,804:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:35,840:INFO:Calculating mean and std
2025-06-03 11:52:35,840:INFO:Creating metrics dataframe
2025-06-03 11:52:35,846:INFO:Uploading results into container
2025-06-03 11:52:35,846:INFO:Uploading model into container now
2025-06-03 11:52:35,857:INFO:_master_model_container: 6
2025-06-03 11:52:35,857:INFO:_display_container: 2
2025-06-03 11:52:35,857:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5773, solver='auto',
                tol=0.0001)
2025-06-03 11:52:35,857:INFO:create_model() successfully completed......................................
2025-06-03 11:52:36,176:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:36,176:INFO:Creating metrics dataframe
2025-06-03 11:52:36,202:INFO:Initializing Random Forest Classifier
2025-06-03 11:52:36,202:INFO:Total runtime is 0.4041821241378784 minutes
2025-06-03 11:52:36,259:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:36,259:INFO:Initializing create_model()
2025-06-03 11:52:36,259:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:36,259:INFO:Checking exceptions
2025-06-03 11:52:36,259:INFO:Importing libraries
2025-06-03 11:52:36,267:INFO:Copying training dataset
2025-06-03 11:52:36,515:INFO:Defining folds
2025-06-03 11:52:36,515:INFO:Declaring metric variables
2025-06-03 11:52:36,554:INFO:Importing untrained model
2025-06-03 11:52:36,600:INFO:Random Forest Classifier Imported successfully
2025-06-03 11:52:36,695:INFO:Starting cross validation
2025-06-03 11:52:36,703:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:40,301:INFO:Calculating mean and std
2025-06-03 11:52:40,301:INFO:Creating metrics dataframe
2025-06-03 11:52:40,301:INFO:Uploading results into container
2025-06-03 11:52:40,301:INFO:Uploading model into container now
2025-06-03 11:52:40,301:INFO:_master_model_container: 7
2025-06-03 11:52:40,301:INFO:_display_container: 2
2025-06-03 11:52:40,313:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5773, verbose=0,
                       warm_start=False)
2025-06-03 11:52:40,313:INFO:create_model() successfully completed......................................
2025-06-03 11:52:40,459:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:40,461:INFO:Creating metrics dataframe
2025-06-03 11:52:40,480:INFO:Initializing Quadratic Discriminant Analysis
2025-06-03 11:52:40,480:INFO:Total runtime is 0.4754754900932312 minutes
2025-06-03 11:52:40,483:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:40,483:INFO:Initializing create_model()
2025-06-03 11:52:40,491:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:40,491:INFO:Checking exceptions
2025-06-03 11:52:40,491:INFO:Importing libraries
2025-06-03 11:52:40,491:INFO:Copying training dataset
2025-06-03 11:52:40,533:INFO:Defining folds
2025-06-03 11:52:40,533:INFO:Declaring metric variables
2025-06-03 11:52:40,539:INFO:Importing untrained model
2025-06-03 11:52:40,547:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-03 11:52:40,571:INFO:Starting cross validation
2025-06-03 11:52:40,573:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:40,684:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,686:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,686:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,700:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,771:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:40,773:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:40,773:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:40,785:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:40,790:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:40,792:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:40,869:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,877:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,891:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:40,913:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:41,024:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,032:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,046:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,054:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,056:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,070:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,072:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,117:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,163:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:41,169:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 11:52:41,222:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,222:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:41,234:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,240:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:41,330:INFO:Calculating mean and std
2025-06-03 11:52:41,363:INFO:Creating metrics dataframe
2025-06-03 11:52:41,390:INFO:Uploading results into container
2025-06-03 11:52:41,406:INFO:Uploading model into container now
2025-06-03 11:52:41,428:INFO:_master_model_container: 8
2025-06-03 11:52:41,428:INFO:_display_container: 2
2025-06-03 11:52:41,428:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-03 11:52:41,428:INFO:create_model() successfully completed......................................
2025-06-03 11:52:41,649:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:41,649:INFO:Creating metrics dataframe
2025-06-03 11:52:41,716:INFO:Initializing Ada Boost Classifier
2025-06-03 11:52:41,718:INFO:Total runtime is 0.4961185812950134 minutes
2025-06-03 11:52:41,747:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:41,751:INFO:Initializing create_model()
2025-06-03 11:52:41,751:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:41,751:INFO:Checking exceptions
2025-06-03 11:52:41,751:INFO:Importing libraries
2025-06-03 11:52:41,751:INFO:Copying training dataset
2025-06-03 11:52:41,823:INFO:Defining folds
2025-06-03 11:52:41,823:INFO:Declaring metric variables
2025-06-03 11:52:41,846:INFO:Importing untrained model
2025-06-03 11:52:41,864:INFO:Ada Boost Classifier Imported successfully
2025-06-03 11:52:41,959:INFO:Starting cross validation
2025-06-03 11:52:41,967:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:52:42,175:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:42,191:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:42,201:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:42,218:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:42,947:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:42,969:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:42,975:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:42,991:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:43,027:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:43,114:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:43,126:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:43,138:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:43,161:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:43,786:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:43,792:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:43,802:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:43,804:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:43,806:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:43,820:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:43,841:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:43,870:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:43,879:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 11:52:44,323:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:44,339:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:52:44,339:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:52:44,370:INFO:Calculating mean and std
2025-06-03 11:52:44,375:INFO:Creating metrics dataframe
2025-06-03 11:52:44,375:INFO:Uploading results into container
2025-06-03 11:52:44,375:INFO:Uploading model into container now
2025-06-03 11:52:44,375:INFO:_master_model_container: 9
2025-06-03 11:52:44,375:INFO:_display_container: 2
2025-06-03 11:52:44,375:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5773)
2025-06-03 11:52:44,375:INFO:create_model() successfully completed......................................
2025-06-03 11:52:44,524:INFO:SubProcess create_model() end ==================================
2025-06-03 11:52:44,524:INFO:Creating metrics dataframe
2025-06-03 11:52:44,538:INFO:Initializing Gradient Boosting Classifier
2025-06-03 11:52:44,538:INFO:Total runtime is 0.5431210279464721 minutes
2025-06-03 11:52:44,545:INFO:SubProcess create_model() called ==================================
2025-06-03 11:52:44,545:INFO:Initializing create_model()
2025-06-03 11:52:44,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:52:44,554:INFO:Checking exceptions
2025-06-03 11:52:44,554:INFO:Importing libraries
2025-06-03 11:52:44,554:INFO:Copying training dataset
2025-06-03 11:52:44,593:INFO:Defining folds
2025-06-03 11:52:44,593:INFO:Declaring metric variables
2025-06-03 11:52:44,608:INFO:Importing untrained model
2025-06-03 11:52:44,610:INFO:Gradient Boosting Classifier Imported successfully
2025-06-03 11:52:44,630:INFO:Starting cross validation
2025-06-03 11:52:44,637:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:53:01,958:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:01,986:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:02,096:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:02,148:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:16,133:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:16,228:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:16,277:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:16,374:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:29,890:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:29,939:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:29,969:INFO:Calculating mean and std
2025-06-03 11:53:29,973:INFO:Creating metrics dataframe
2025-06-03 11:53:29,984:INFO:Uploading results into container
2025-06-03 11:53:29,985:INFO:Uploading model into container now
2025-06-03 11:53:29,985:INFO:_master_model_container: 10
2025-06-03 11:53:29,985:INFO:_display_container: 2
2025-06-03 11:53:29,987:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5773, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-03 11:53:29,987:INFO:create_model() successfully completed......................................
2025-06-03 11:53:30,128:INFO:SubProcess create_model() end ==================================
2025-06-03 11:53:30,128:INFO:Creating metrics dataframe
2025-06-03 11:53:30,149:INFO:Initializing Linear Discriminant Analysis
2025-06-03 11:53:30,149:INFO:Total runtime is 1.303295071919759 minutes
2025-06-03 11:53:30,164:INFO:SubProcess create_model() called ==================================
2025-06-03 11:53:30,164:INFO:Initializing create_model()
2025-06-03 11:53:30,164:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:53:30,164:INFO:Checking exceptions
2025-06-03 11:53:30,164:INFO:Importing libraries
2025-06-03 11:53:30,164:INFO:Copying training dataset
2025-06-03 11:53:30,208:INFO:Defining folds
2025-06-03 11:53:30,208:INFO:Declaring metric variables
2025-06-03 11:53:30,208:INFO:Importing untrained model
2025-06-03 11:53:30,226:INFO:Linear Discriminant Analysis Imported successfully
2025-06-03 11:53:30,249:INFO:Starting cross validation
2025-06-03 11:53:30,249:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:53:30,377:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,379:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,385:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,504:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,512:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,520:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,530:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,613:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,616:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 11:53:30,653:INFO:Calculating mean and std
2025-06-03 11:53:30,658:INFO:Creating metrics dataframe
2025-06-03 11:53:30,661:INFO:Uploading results into container
2025-06-03 11:53:30,661:INFO:Uploading model into container now
2025-06-03 11:53:30,669:INFO:_master_model_container: 11
2025-06-03 11:53:30,669:INFO:_display_container: 2
2025-06-03 11:53:30,669:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-03 11:53:30,669:INFO:create_model() successfully completed......................................
2025-06-03 11:53:30,812:INFO:SubProcess create_model() end ==================================
2025-06-03 11:53:30,812:INFO:Creating metrics dataframe
2025-06-03 11:53:30,827:INFO:Initializing Extra Trees Classifier
2025-06-03 11:53:30,827:INFO:Total runtime is 1.3146041711171466 minutes
2025-06-03 11:53:30,843:INFO:SubProcess create_model() called ==================================
2025-06-03 11:53:30,846:INFO:Initializing create_model()
2025-06-03 11:53:30,846:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:53:30,846:INFO:Checking exceptions
2025-06-03 11:53:30,846:INFO:Importing libraries
2025-06-03 11:53:30,846:INFO:Copying training dataset
2025-06-03 11:53:30,879:INFO:Defining folds
2025-06-03 11:53:30,887:INFO:Declaring metric variables
2025-06-03 11:53:30,893:INFO:Importing untrained model
2025-06-03 11:53:30,893:INFO:Extra Trees Classifier Imported successfully
2025-06-03 11:53:30,933:INFO:Starting cross validation
2025-06-03 11:53:30,935:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:53:33,994:INFO:Calculating mean and std
2025-06-03 11:53:33,994:INFO:Creating metrics dataframe
2025-06-03 11:53:33,994:INFO:Uploading results into container
2025-06-03 11:53:33,994:INFO:Uploading model into container now
2025-06-03 11:53:33,994:INFO:_master_model_container: 12
2025-06-03 11:53:33,994:INFO:_display_container: 2
2025-06-03 11:53:33,994:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5773, verbose=0,
                     warm_start=False)
2025-06-03 11:53:33,994:INFO:create_model() successfully completed......................................
2025-06-03 11:53:34,137:INFO:SubProcess create_model() end ==================================
2025-06-03 11:53:34,137:INFO:Creating metrics dataframe
2025-06-03 11:53:34,169:INFO:Initializing Light Gradient Boosting Machine
2025-06-03 11:53:34,169:INFO:Total runtime is 1.3703033288319904 minutes
2025-06-03 11:53:34,181:INFO:SubProcess create_model() called ==================================
2025-06-03 11:53:34,181:INFO:Initializing create_model()
2025-06-03 11:53:34,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:53:34,181:INFO:Checking exceptions
2025-06-03 11:53:34,181:INFO:Importing libraries
2025-06-03 11:53:34,181:INFO:Copying training dataset
2025-06-03 11:53:34,225:INFO:Defining folds
2025-06-03 11:53:34,225:INFO:Declaring metric variables
2025-06-03 11:53:34,241:INFO:Importing untrained model
2025-06-03 11:53:34,252:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-03 11:53:34,336:INFO:Starting cross validation
2025-06-03 11:53:34,340:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:53:55,425:INFO:Calculating mean and std
2025-06-03 11:53:55,431:INFO:Creating metrics dataframe
2025-06-03 11:53:55,439:INFO:Uploading results into container
2025-06-03 11:53:55,441:INFO:Uploading model into container now
2025-06-03 11:53:55,443:INFO:_master_model_container: 13
2025-06-03 11:53:55,445:INFO:_display_container: 2
2025-06-03 11:53:55,447:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5773, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-03 11:53:55,447:INFO:create_model() successfully completed......................................
2025-06-03 11:53:55,617:INFO:SubProcess create_model() end ==================================
2025-06-03 11:53:55,617:INFO:Creating metrics dataframe
2025-06-03 11:53:55,645:INFO:Initializing Dummy Classifier
2025-06-03 11:53:55,645:INFO:Total runtime is 1.7282270630200702 minutes
2025-06-03 11:53:55,654:INFO:SubProcess create_model() called ==================================
2025-06-03 11:53:55,654:INFO:Initializing create_model()
2025-06-03 11:53:55,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023DBF19FFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:53:55,654:INFO:Checking exceptions
2025-06-03 11:53:55,654:INFO:Importing libraries
2025-06-03 11:53:55,654:INFO:Copying training dataset
2025-06-03 11:53:55,700:INFO:Defining folds
2025-06-03 11:53:55,700:INFO:Declaring metric variables
2025-06-03 11:53:55,717:INFO:Importing untrained model
2025-06-03 11:53:55,724:INFO:Dummy Classifier Imported successfully
2025-06-03 11:53:55,743:INFO:Starting cross validation
2025-06-03 11:53:55,749:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 11:53:55,903:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:55,911:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:55,919:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:55,941:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,044:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,055:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,064:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,077:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,161:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,174:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 11:53:56,199:INFO:Calculating mean and std
2025-06-03 11:53:56,201:INFO:Creating metrics dataframe
2025-06-03 11:53:56,212:INFO:Uploading results into container
2025-06-03 11:53:56,217:INFO:Uploading model into container now
2025-06-03 11:53:56,217:INFO:_master_model_container: 14
2025-06-03 11:53:56,217:INFO:_display_container: 2
2025-06-03 11:53:56,217:INFO:DummyClassifier(constant=None, random_state=5773, strategy='prior')
2025-06-03 11:53:56,217:INFO:create_model() successfully completed......................................
2025-06-03 11:53:56,455:INFO:SubProcess create_model() end ==================================
2025-06-03 11:53:56,455:INFO:Creating metrics dataframe
2025-06-03 11:53:56,492:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-03 11:53:56,522:INFO:Initializing create_model()
2025-06-03 11:53:56,522:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023DC23C46A0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 11:53:56,522:INFO:Checking exceptions
2025-06-03 11:53:56,532:INFO:Importing libraries
2025-06-03 11:53:56,532:INFO:Copying training dataset
2025-06-03 11:53:56,579:INFO:Defining folds
2025-06-03 11:53:56,579:INFO:Declaring metric variables
2025-06-03 11:53:56,579:INFO:Importing untrained model
2025-06-03 11:53:56,579:INFO:Declaring custom model
2025-06-03 11:53:56,579:INFO:K Neighbors Classifier Imported successfully
2025-06-03 11:53:56,579:INFO:Cross validation set to False
2025-06-03 11:53:56,579:INFO:Fitting Model
2025-06-03 11:53:56,625:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:53:56,625:INFO:create_model() successfully completed......................................
2025-06-03 11:53:56,872:INFO:_master_model_container: 14
2025-06-03 11:53:56,872:INFO:_display_container: 2
2025-06-03 11:53:56,872:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 11:53:56,872:INFO:compare_models() successfully completed......................................
2025-06-03 12:10:33,314:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 12:10:33,329:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 12:10:33,329:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 12:10:33,331:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-03 12:10:36,719:INFO:PyCaret ClassificationExperiment
2025-06-03 12:10:36,721:INFO:Logging name: clf-default-name
2025-06-03 12:10:36,723:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-03 12:10:36,723:INFO:version 3.3.2
2025-06-03 12:10:36,725:INFO:Initializing setup()
2025-06-03 12:10:36,725:INFO:self.USI: 75f1
2025-06-03 12:10:36,725:INFO:self._variable_keys: {'data', '_ml_usecase', 'y_train', 'target_param', 'USI', '_available_plots', 'fold_groups_param', 'X_train', 'y', 'logging_param', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'memory', 'fix_imbalance', 'gpu_param', 'html_param', 'idx', 'X', 'gpu_n_jobs_param', 'exp_id', 'fold_generator', 'is_multiclass', 'seed', 'fold_shuffle_param', 'y_test', 'X_test', 'pipeline'}
2025-06-03 12:10:36,729:INFO:Checking environment
2025-06-03 12:10:36,729:INFO:python_version: 3.10.0
2025-06-03 12:10:36,733:INFO:python_build: ('tags/v3.10.0:b494f59', 'Oct  4 2021 19:00:18')
2025-06-03 12:10:36,735:INFO:machine: AMD64
2025-06-03 12:10:36,735:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-03 12:10:36,749:INFO:Memory: svmem(total=4110286848, available=450273280, percent=89.0, used=3660013568, free=450273280)
2025-06-03 12:10:36,755:INFO:Physical Core: 4
2025-06-03 12:10:36,755:INFO:Logical Core: 4
2025-06-03 12:10:36,757:INFO:Checking libraries
2025-06-03 12:10:36,759:INFO:System:
2025-06-03 12:10:36,762:INFO:    python: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
2025-06-03 12:10:36,764:INFO:executable: c:\Program Files\Python310\python.exe
2025-06-03 12:10:36,764:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-03 12:10:36,764:INFO:PyCaret required dependencies:
2025-06-03 12:10:37,169:INFO:                 pip: 21.2.3
2025-06-03 12:10:37,169:INFO:          setuptools: 57.4.0
2025-06-03 12:10:37,169:INFO:             pycaret: 3.3.2
2025-06-03 12:10:37,171:INFO:             IPython: 8.30.0
2025-06-03 12:10:37,171:INFO:          ipywidgets: 8.1.5
2025-06-03 12:10:37,171:INFO:                tqdm: 4.67.1
2025-06-03 12:10:37,171:INFO:               numpy: 1.26.4
2025-06-03 12:10:37,173:INFO:              pandas: 2.1.4
2025-06-03 12:10:37,173:INFO:              jinja2: 3.1.4
2025-06-03 12:10:37,173:INFO:               scipy: 1.11.4
2025-06-03 12:10:37,173:INFO:              joblib: 1.3.2
2025-06-03 12:10:37,173:INFO:             sklearn: 1.4.2
2025-06-03 12:10:37,175:INFO:                pyod: 2.0.5
2025-06-03 12:10:37,175:INFO:            imblearn: 0.13.0
2025-06-03 12:10:37,175:INFO:   category_encoders: 2.7.0
2025-06-03 12:10:37,175:INFO:            lightgbm: 4.6.0
2025-06-03 12:10:37,175:INFO:               numba: 0.61.2
2025-06-03 12:10:37,177:INFO:            requests: 2.32.3
2025-06-03 12:10:37,177:INFO:          matplotlib: 3.7.5
2025-06-03 12:10:37,177:INFO:          scikitplot: 0.3.7
2025-06-03 12:10:37,177:INFO:         yellowbrick: 1.5
2025-06-03 12:10:37,177:INFO:              plotly: 5.24.1
2025-06-03 12:10:37,179:INFO:    plotly-resampler: Not installed
2025-06-03 12:10:37,179:INFO:             kaleido: 0.2.1
2025-06-03 12:10:37,179:INFO:           schemdraw: 0.15
2025-06-03 12:10:37,179:INFO:         statsmodels: 0.14.4
2025-06-03 12:10:37,179:INFO:              sktime: 0.26.0
2025-06-03 12:10:37,179:INFO:               tbats: 1.1.3
2025-06-03 12:10:37,179:INFO:            pmdarima: 2.0.4
2025-06-03 12:10:37,179:INFO:              psutil: 6.1.0
2025-06-03 12:10:37,179:INFO:          markupsafe: 3.0.2
2025-06-03 12:10:37,181:INFO:             pickle5: Not installed
2025-06-03 12:10:37,181:INFO:         cloudpickle: 3.1.1
2025-06-03 12:10:37,181:INFO:         deprecation: 2.1.0
2025-06-03 12:10:37,181:INFO:              xxhash: 3.5.0
2025-06-03 12:10:37,181:INFO:           wurlitzer: Not installed
2025-06-03 12:10:37,181:INFO:PyCaret optional dependencies:
2025-06-03 12:10:37,236:INFO:                shap: Not installed
2025-06-03 12:10:37,236:INFO:           interpret: Not installed
2025-06-03 12:10:37,236:INFO:                umap: Not installed
2025-06-03 12:10:37,236:INFO:     ydata_profiling: Not installed
2025-06-03 12:10:37,236:INFO:  explainerdashboard: Not installed
2025-06-03 12:10:37,236:INFO:             autoviz: Not installed
2025-06-03 12:10:37,236:INFO:           fairlearn: Not installed
2025-06-03 12:10:37,236:INFO:          deepchecks: Not installed
2025-06-03 12:10:37,236:INFO:             xgboost: Not installed
2025-06-03 12:10:37,236:INFO:            catboost: Not installed
2025-06-03 12:10:37,236:INFO:              kmodes: Not installed
2025-06-03 12:10:37,236:INFO:             mlxtend: Not installed
2025-06-03 12:10:37,238:INFO:       statsforecast: Not installed
2025-06-03 12:10:37,238:INFO:        tune_sklearn: Not installed
2025-06-03 12:10:37,238:INFO:                 ray: Not installed
2025-06-03 12:10:37,238:INFO:            hyperopt: Not installed
2025-06-03 12:10:37,238:INFO:              optuna: Not installed
2025-06-03 12:10:37,238:INFO:               skopt: Not installed
2025-06-03 12:10:37,238:INFO:              mlflow: Not installed
2025-06-03 12:10:37,238:INFO:              gradio: Not installed
2025-06-03 12:10:37,238:INFO:             fastapi: Not installed
2025-06-03 12:10:37,238:INFO:             uvicorn: Not installed
2025-06-03 12:10:37,238:INFO:              m2cgen: Not installed
2025-06-03 12:10:37,238:INFO:           evidently: Not installed
2025-06-03 12:10:37,238:INFO:               fugue: Not installed
2025-06-03 12:10:37,240:INFO:           streamlit: 1.44.1
2025-06-03 12:10:37,240:INFO:             prophet: Not installed
2025-06-03 12:10:37,240:INFO:None
2025-06-03 12:10:37,240:INFO:Set up data.
2025-06-03 12:10:37,299:INFO:Set up folding strategy.
2025-06-03 12:10:37,299:INFO:Set up train/test split.
2025-06-03 12:10:37,366:INFO:Set up index.
2025-06-03 12:10:37,369:INFO:Assigning column types.
2025-06-03 12:10:37,413:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-03 12:10:37,646:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 12:10:37,652:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 12:10:37,856:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:37,860:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,045:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-03 12:10:38,055:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 12:10:38,164:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,164:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,166:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-03 12:10:38,365:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 12:10:38,467:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,467:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,659:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-03 12:10:38,770:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:38,772:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-03 12:10:39,069:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:39,069:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:39,352:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:39,352:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:39,360:INFO:Preparing preprocessing pipeline...
2025-06-03 12:10:39,367:INFO:Set up simple imputation.
2025-06-03 12:10:39,482:INFO:Finished creating preprocessing pipeline.
2025-06-03 12:10:39,495:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\NEVILL~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['pixel_0_0', 'pixel_0_1',
                                             'pixel_0_2', 'pixel_0_3',
                                             'pixel_0_4', 'pixel_0_5',
                                             'pixel_0_6', 'pixel_0_7',
                                             'pixel_1_0', 'pixel_1_1',
                                             'pixel_1_2', 'pixel_1_3',
                                             'pixel_1_4', 'pixel_1_5',
                                             'pixel_1_6', 'pixel_1_7',
                                             'pixel_2...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-06-03 12:10:39,495:INFO:Creating final display dataframe.
2025-06-03 12:10:40,051:INFO:Setup _display_container:                     Description             Value
0                    Session id              5898
1                        Target            target
2                   Target type        Multiclass
3           Original data shape        (1797, 65)
4        Transformed data shape        (1797, 65)
5   Transformed train set shape        (1257, 65)
6    Transformed test set shape         (540, 65)
7              Numeric features                64
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              75f1
2025-06-03 12:10:40,400:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:40,400:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:40,669:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:40,669:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-03 12:10:40,669:INFO:setup() successfully completed in 3.98s...............
2025-06-03 12:10:40,701:INFO:Initializing compare_models()
2025-06-03 12:10:40,711:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-03 12:10:40,713:INFO:Checking exceptions
2025-06-03 12:10:40,753:INFO:Preparing display monitor
2025-06-03 12:10:40,828:INFO:Initializing Logistic Regression
2025-06-03 12:10:40,828:INFO:Total runtime is 0.0 minutes
2025-06-03 12:10:40,844:INFO:SubProcess create_model() called ==================================
2025-06-03 12:10:40,844:INFO:Initializing create_model()
2025-06-03 12:10:40,844:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:10:40,844:INFO:Checking exceptions
2025-06-03 12:10:40,844:INFO:Importing libraries
2025-06-03 12:10:40,844:INFO:Copying training dataset
2025-06-03 12:10:40,892:INFO:Defining folds
2025-06-03 12:10:40,892:INFO:Declaring metric variables
2025-06-03 12:10:40,895:INFO:Importing untrained model
2025-06-03 12:10:40,909:INFO:Logistic Regression Imported successfully
2025-06-03 12:10:40,932:INFO:Starting cross validation
2025-06-03 12:10:40,932:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:10:56,817:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,052:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,097:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,298:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,574:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,803:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:57,841:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:58,016:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:58,101:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:58,254:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:10:58,286:INFO:Calculating mean and std
2025-06-03 12:10:58,292:INFO:Creating metrics dataframe
2025-06-03 12:10:58,301:INFO:Uploading results into container
2025-06-03 12:10:58,302:INFO:Uploading model into container now
2025-06-03 12:10:58,302:INFO:_master_model_container: 1
2025-06-03 12:10:58,302:INFO:_display_container: 2
2025-06-03 12:10:58,302:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5898, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-03 12:10:58,302:INFO:create_model() successfully completed......................................
2025-06-03 12:10:58,541:INFO:SubProcess create_model() end ==================================
2025-06-03 12:10:58,541:INFO:Creating metrics dataframe
2025-06-03 12:10:58,563:INFO:Initializing K Neighbors Classifier
2025-06-03 12:10:58,563:INFO:Total runtime is 0.2955699841181437 minutes
2025-06-03 12:10:58,575:INFO:SubProcess create_model() called ==================================
2025-06-03 12:10:58,575:INFO:Initializing create_model()
2025-06-03 12:10:58,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:10:58,575:INFO:Checking exceptions
2025-06-03 12:10:58,575:INFO:Importing libraries
2025-06-03 12:10:58,575:INFO:Copying training dataset
2025-06-03 12:10:58,620:INFO:Defining folds
2025-06-03 12:10:58,620:INFO:Declaring metric variables
2025-06-03 12:10:58,625:INFO:Importing untrained model
2025-06-03 12:10:58,638:INFO:K Neighbors Classifier Imported successfully
2025-06-03 12:10:58,662:INFO:Starting cross validation
2025-06-03 12:10:58,670:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:10:59,507:INFO:Calculating mean and std
2025-06-03 12:10:59,509:INFO:Creating metrics dataframe
2025-06-03 12:10:59,509:INFO:Uploading results into container
2025-06-03 12:10:59,509:INFO:Uploading model into container now
2025-06-03 12:10:59,509:INFO:_master_model_container: 2
2025-06-03 12:10:59,509:INFO:_display_container: 2
2025-06-03 12:10:59,509:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 12:10:59,509:INFO:create_model() successfully completed......................................
2025-06-03 12:10:59,713:INFO:SubProcess create_model() end ==================================
2025-06-03 12:10:59,713:INFO:Creating metrics dataframe
2025-06-03 12:10:59,732:INFO:Initializing Naive Bayes
2025-06-03 12:10:59,732:INFO:Total runtime is 0.31505836645762125 minutes
2025-06-03 12:10:59,736:INFO:SubProcess create_model() called ==================================
2025-06-03 12:10:59,744:INFO:Initializing create_model()
2025-06-03 12:10:59,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:10:59,744:INFO:Checking exceptions
2025-06-03 12:10:59,744:INFO:Importing libraries
2025-06-03 12:10:59,744:INFO:Copying training dataset
2025-06-03 12:10:59,799:INFO:Defining folds
2025-06-03 12:10:59,799:INFO:Declaring metric variables
2025-06-03 12:10:59,807:INFO:Importing untrained model
2025-06-03 12:10:59,813:INFO:Naive Bayes Imported successfully
2025-06-03 12:10:59,839:INFO:Starting cross validation
2025-06-03 12:10:59,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:00,472:INFO:Calculating mean and std
2025-06-03 12:11:00,480:INFO:Creating metrics dataframe
2025-06-03 12:11:00,505:INFO:Uploading results into container
2025-06-03 12:11:00,523:INFO:Uploading model into container now
2025-06-03 12:11:00,533:INFO:_master_model_container: 3
2025-06-03 12:11:00,533:INFO:_display_container: 2
2025-06-03 12:11:00,533:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-03 12:11:00,533:INFO:create_model() successfully completed......................................
2025-06-03 12:11:00,811:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:00,811:INFO:Creating metrics dataframe
2025-06-03 12:11:00,833:INFO:Initializing Decision Tree Classifier
2025-06-03 12:11:00,835:INFO:Total runtime is 0.33340953588485717 minutes
2025-06-03 12:11:00,854:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:00,856:INFO:Initializing create_model()
2025-06-03 12:11:00,856:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:00,856:INFO:Checking exceptions
2025-06-03 12:11:00,856:INFO:Importing libraries
2025-06-03 12:11:00,856:INFO:Copying training dataset
2025-06-03 12:11:00,900:INFO:Defining folds
2025-06-03 12:11:00,900:INFO:Declaring metric variables
2025-06-03 12:11:00,921:INFO:Importing untrained model
2025-06-03 12:11:00,933:INFO:Decision Tree Classifier Imported successfully
2025-06-03 12:11:00,960:INFO:Starting cross validation
2025-06-03 12:11:00,968:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:01,704:INFO:Calculating mean and std
2025-06-03 12:11:01,710:INFO:Creating metrics dataframe
2025-06-03 12:11:01,735:INFO:Uploading results into container
2025-06-03 12:11:01,738:INFO:Uploading model into container now
2025-06-03 12:11:01,744:INFO:_master_model_container: 4
2025-06-03 12:11:01,744:INFO:_display_container: 2
2025-06-03 12:11:01,746:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5898, splitter='best')
2025-06-03 12:11:01,746:INFO:create_model() successfully completed......................................
2025-06-03 12:11:01,942:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:01,942:INFO:Creating metrics dataframe
2025-06-03 12:11:01,960:INFO:Initializing SVM - Linear Kernel
2025-06-03 12:11:01,960:INFO:Total runtime is 0.35218976736068724 minutes
2025-06-03 12:11:01,969:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:01,971:INFO:Initializing create_model()
2025-06-03 12:11:01,971:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:01,971:INFO:Checking exceptions
2025-06-03 12:11:01,971:INFO:Importing libraries
2025-06-03 12:11:01,971:INFO:Copying training dataset
2025-06-03 12:11:02,021:INFO:Defining folds
2025-06-03 12:11:02,021:INFO:Declaring metric variables
2025-06-03 12:11:02,035:INFO:Importing untrained model
2025-06-03 12:11:02,071:INFO:SVM - Linear Kernel Imported successfully
2025-06-03 12:11:02,089:INFO:Starting cross validation
2025-06-03 12:11:02,098:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:02,369:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,379:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,399:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,399:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,608:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,614:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,636:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,646:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,950:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:02,969:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:03,017:INFO:Calculating mean and std
2025-06-03 12:11:03,020:INFO:Creating metrics dataframe
2025-06-03 12:11:03,020:INFO:Uploading results into container
2025-06-03 12:11:03,020:INFO:Uploading model into container now
2025-06-03 12:11:03,020:INFO:_master_model_container: 5
2025-06-03 12:11:03,020:INFO:_display_container: 2
2025-06-03 12:11:03,020:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5898, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-03 12:11:03,031:INFO:create_model() successfully completed......................................
2025-06-03 12:11:03,428:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:03,428:INFO:Creating metrics dataframe
2025-06-03 12:11:03,469:INFO:Initializing Ridge Classifier
2025-06-03 12:11:03,471:INFO:Total runtime is 0.3773850917816162 minutes
2025-06-03 12:11:03,487:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:03,493:INFO:Initializing create_model()
2025-06-03 12:11:03,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:03,493:INFO:Checking exceptions
2025-06-03 12:11:03,493:INFO:Importing libraries
2025-06-03 12:11:03,493:INFO:Copying training dataset
2025-06-03 12:11:03,562:INFO:Defining folds
2025-06-03 12:11:03,562:INFO:Declaring metric variables
2025-06-03 12:11:03,579:INFO:Importing untrained model
2025-06-03 12:11:03,586:INFO:Ridge Classifier Imported successfully
2025-06-03 12:11:03,617:INFO:Starting cross validation
2025-06-03 12:11:03,623:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:03,889:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:03,891:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:03,891:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:03,903:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,154:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,211:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,213:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,243:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,360:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,388:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:04,440:INFO:Calculating mean and std
2025-06-03 12:11:04,449:INFO:Creating metrics dataframe
2025-06-03 12:11:04,488:INFO:Uploading results into container
2025-06-03 12:11:04,500:INFO:Uploading model into container now
2025-06-03 12:11:04,510:INFO:_master_model_container: 6
2025-06-03 12:11:04,510:INFO:_display_container: 2
2025-06-03 12:11:04,520:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5898, solver='auto',
                tol=0.0001)
2025-06-03 12:11:04,520:INFO:create_model() successfully completed......................................
2025-06-03 12:11:05,026:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:05,026:INFO:Creating metrics dataframe
2025-06-03 12:11:05,058:INFO:Initializing Random Forest Classifier
2025-06-03 12:11:05,061:INFO:Total runtime is 0.4038831909497579 minutes
2025-06-03 12:11:05,061:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:05,061:INFO:Initializing create_model()
2025-06-03 12:11:05,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:05,061:INFO:Checking exceptions
2025-06-03 12:11:05,061:INFO:Importing libraries
2025-06-03 12:11:05,061:INFO:Copying training dataset
2025-06-03 12:11:05,106:INFO:Defining folds
2025-06-03 12:11:05,106:INFO:Declaring metric variables
2025-06-03 12:11:05,124:INFO:Importing untrained model
2025-06-03 12:11:05,128:INFO:Random Forest Classifier Imported successfully
2025-06-03 12:11:05,150:INFO:Starting cross validation
2025-06-03 12:11:05,156:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:09,144:INFO:Calculating mean and std
2025-06-03 12:11:09,146:INFO:Creating metrics dataframe
2025-06-03 12:11:09,151:INFO:Uploading results into container
2025-06-03 12:11:09,151:INFO:Uploading model into container now
2025-06-03 12:11:09,151:INFO:_master_model_container: 7
2025-06-03 12:11:09,151:INFO:_display_container: 2
2025-06-03 12:11:09,151:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5898, verbose=0,
                       warm_start=False)
2025-06-03 12:11:09,151:INFO:create_model() successfully completed......................................
2025-06-03 12:11:09,307:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:09,307:INFO:Creating metrics dataframe
2025-06-03 12:11:09,325:INFO:Initializing Quadratic Discriminant Analysis
2025-06-03 12:11:09,325:INFO:Total runtime is 0.47493816614151 minutes
2025-06-03 12:11:09,337:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:09,337:INFO:Initializing create_model()
2025-06-03 12:11:09,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:09,340:INFO:Checking exceptions
2025-06-03 12:11:09,340:INFO:Importing libraries
2025-06-03 12:11:09,340:INFO:Copying training dataset
2025-06-03 12:11:09,382:INFO:Defining folds
2025-06-03 12:11:09,382:INFO:Declaring metric variables
2025-06-03 12:11:09,404:INFO:Importing untrained model
2025-06-03 12:11:09,420:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-03 12:11:09,443:INFO:Starting cross validation
2025-06-03 12:11:09,447:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:09,607:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:09,611:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:09,627:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:09,707:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:09,711:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:09,714:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:09,722:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:09,778:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:10,033:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:10,051:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:10,089:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:10,180:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,194:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,200:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,223:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,336:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:10,344:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-03 12:11:10,408:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,417:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:10,458:INFO:Calculating mean and std
2025-06-03 12:11:10,458:INFO:Creating metrics dataframe
2025-06-03 12:11:10,484:INFO:Uploading results into container
2025-06-03 12:11:10,486:INFO:Uploading model into container now
2025-06-03 12:11:10,488:INFO:_master_model_container: 8
2025-06-03 12:11:10,488:INFO:_display_container: 2
2025-06-03 12:11:10,488:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-03 12:11:10,488:INFO:create_model() successfully completed......................................
2025-06-03 12:11:10,673:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:10,673:INFO:Creating metrics dataframe
2025-06-03 12:11:10,695:INFO:Initializing Ada Boost Classifier
2025-06-03 12:11:10,696:INFO:Total runtime is 0.4978014945983887 minutes
2025-06-03 12:11:10,708:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:10,708:INFO:Initializing create_model()
2025-06-03 12:11:10,708:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:10,708:INFO:Checking exceptions
2025-06-03 12:11:10,708:INFO:Importing libraries
2025-06-03 12:11:10,708:INFO:Copying training dataset
2025-06-03 12:11:10,760:INFO:Defining folds
2025-06-03 12:11:10,760:INFO:Declaring metric variables
2025-06-03 12:11:10,769:INFO:Importing untrained model
2025-06-03 12:11:10,784:INFO:Ada Boost Classifier Imported successfully
2025-06-03 12:11:10,808:INFO:Starting cross validation
2025-06-03 12:11:10,812:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:10,926:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:10,938:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:10,950:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:10,958:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:11,531:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:11,538:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:11,544:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:11,550:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:11,554:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:11,562:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:11,576:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:11,590:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:11,633:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:11,649:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:11,661:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:11,671:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:12,264:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,270:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,274:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,278:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,281:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,285:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,293:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,319:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,357:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:12,361:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-03 12:11:12,772:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,772:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:12,790:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,790:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:11:12,819:INFO:Calculating mean and std
2025-06-03 12:11:12,823:INFO:Creating metrics dataframe
2025-06-03 12:11:12,826:INFO:Uploading results into container
2025-06-03 12:11:12,828:INFO:Uploading model into container now
2025-06-03 12:11:12,830:INFO:_master_model_container: 9
2025-06-03 12:11:12,832:INFO:_display_container: 2
2025-06-03 12:11:12,832:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5898)
2025-06-03 12:11:12,832:INFO:create_model() successfully completed......................................
2025-06-03 12:11:12,983:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:12,983:INFO:Creating metrics dataframe
2025-06-03 12:11:13,012:INFO:Initializing Gradient Boosting Classifier
2025-06-03 12:11:13,012:INFO:Total runtime is 0.5363964994748434 minutes
2025-06-03 12:11:13,017:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:13,017:INFO:Initializing create_model()
2025-06-03 12:11:13,017:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:13,017:INFO:Checking exceptions
2025-06-03 12:11:13,017:INFO:Importing libraries
2025-06-03 12:11:13,017:INFO:Copying training dataset
2025-06-03 12:11:13,060:INFO:Defining folds
2025-06-03 12:11:13,060:INFO:Declaring metric variables
2025-06-03 12:11:13,074:INFO:Importing untrained model
2025-06-03 12:11:13,085:INFO:Gradient Boosting Classifier Imported successfully
2025-06-03 12:11:13,105:INFO:Starting cross validation
2025-06-03 12:11:13,111:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:28,091:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:28,099:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:28,150:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:28,160:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:41,472:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:41,544:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:41,602:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:41,639:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,308:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,372:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,404:INFO:Calculating mean and std
2025-06-03 12:11:53,406:INFO:Creating metrics dataframe
2025-06-03 12:11:53,412:INFO:Uploading results into container
2025-06-03 12:11:53,414:INFO:Uploading model into container now
2025-06-03 12:11:53,414:INFO:_master_model_container: 10
2025-06-03 12:11:53,414:INFO:_display_container: 2
2025-06-03 12:11:53,414:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5898, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-03 12:11:53,414:INFO:create_model() successfully completed......................................
2025-06-03 12:11:53,552:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:53,560:INFO:Creating metrics dataframe
2025-06-03 12:11:53,581:INFO:Initializing Linear Discriminant Analysis
2025-06-03 12:11:53,581:INFO:Total runtime is 1.212543749809265 minutes
2025-06-03 12:11:53,588:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:53,594:INFO:Initializing create_model()
2025-06-03 12:11:53,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:53,595:INFO:Checking exceptions
2025-06-03 12:11:53,595:INFO:Importing libraries
2025-06-03 12:11:53,595:INFO:Copying training dataset
2025-06-03 12:11:53,678:INFO:Defining folds
2025-06-03 12:11:53,678:INFO:Declaring metric variables
2025-06-03 12:11:53,698:INFO:Importing untrained model
2025-06-03 12:11:53,712:INFO:Linear Discriminant Analysis Imported successfully
2025-06-03 12:11:53,745:INFO:Starting cross validation
2025-06-03 12:11:53,749:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:53,945:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,947:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,953:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:53,961:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,077:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,087:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,105:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,109:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,180:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,180:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-03 12:11:54,228:INFO:Calculating mean and std
2025-06-03 12:11:54,228:INFO:Creating metrics dataframe
2025-06-03 12:11:54,234:INFO:Uploading results into container
2025-06-03 12:11:54,239:INFO:Uploading model into container now
2025-06-03 12:11:54,241:INFO:_master_model_container: 11
2025-06-03 12:11:54,241:INFO:_display_container: 2
2025-06-03 12:11:54,241:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-03 12:11:54,241:INFO:create_model() successfully completed......................................
2025-06-03 12:11:54,371:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:54,371:INFO:Creating metrics dataframe
2025-06-03 12:11:54,406:INFO:Initializing Extra Trees Classifier
2025-06-03 12:11:54,406:INFO:Total runtime is 1.2262933810551961 minutes
2025-06-03 12:11:54,406:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:54,406:INFO:Initializing create_model()
2025-06-03 12:11:54,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:54,406:INFO:Checking exceptions
2025-06-03 12:11:54,406:INFO:Importing libraries
2025-06-03 12:11:54,406:INFO:Copying training dataset
2025-06-03 12:11:54,450:INFO:Defining folds
2025-06-03 12:11:54,450:INFO:Declaring metric variables
2025-06-03 12:11:54,466:INFO:Importing untrained model
2025-06-03 12:11:54,471:INFO:Extra Trees Classifier Imported successfully
2025-06-03 12:11:54,489:INFO:Starting cross validation
2025-06-03 12:11:54,498:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:11:57,200:INFO:Calculating mean and std
2025-06-03 12:11:57,200:INFO:Creating metrics dataframe
2025-06-03 12:11:57,208:INFO:Uploading results into container
2025-06-03 12:11:57,208:INFO:Uploading model into container now
2025-06-03 12:11:57,208:INFO:_master_model_container: 12
2025-06-03 12:11:57,208:INFO:_display_container: 2
2025-06-03 12:11:57,208:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5898, verbose=0,
                     warm_start=False)
2025-06-03 12:11:57,208:INFO:create_model() successfully completed......................................
2025-06-03 12:11:57,437:INFO:SubProcess create_model() end ==================================
2025-06-03 12:11:57,439:INFO:Creating metrics dataframe
2025-06-03 12:11:57,467:INFO:Initializing Light Gradient Boosting Machine
2025-06-03 12:11:57,467:INFO:Total runtime is 1.2773084044456482 minutes
2025-06-03 12:11:57,473:INFO:SubProcess create_model() called ==================================
2025-06-03 12:11:57,473:INFO:Initializing create_model()
2025-06-03 12:11:57,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:11:57,473:INFO:Checking exceptions
2025-06-03 12:11:57,473:INFO:Importing libraries
2025-06-03 12:11:57,473:INFO:Copying training dataset
2025-06-03 12:11:57,532:INFO:Defining folds
2025-06-03 12:11:57,532:INFO:Declaring metric variables
2025-06-03 12:11:57,542:INFO:Importing untrained model
2025-06-03 12:11:57,556:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-03 12:11:57,580:INFO:Starting cross validation
2025-06-03 12:11:57,580:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:12:16,635:INFO:Calculating mean and std
2025-06-03 12:12:16,639:INFO:Creating metrics dataframe
2025-06-03 12:12:16,657:INFO:Uploading results into container
2025-06-03 12:12:16,659:INFO:Uploading model into container now
2025-06-03 12:12:16,661:INFO:_master_model_container: 13
2025-06-03 12:12:16,661:INFO:_display_container: 2
2025-06-03 12:12:16,663:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5898, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-03 12:12:16,663:INFO:create_model() successfully completed......................................
2025-06-03 12:12:16,831:INFO:SubProcess create_model() end ==================================
2025-06-03 12:12:16,831:INFO:Creating metrics dataframe
2025-06-03 12:12:16,848:INFO:Initializing Dummy Classifier
2025-06-03 12:12:16,848:INFO:Total runtime is 1.600328302383423 minutes
2025-06-03 12:12:16,864:INFO:SubProcess create_model() called ==================================
2025-06-03 12:12:16,864:INFO:Initializing create_model()
2025-06-03 12:12:16,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002628B854CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:12:16,864:INFO:Checking exceptions
2025-06-03 12:12:16,864:INFO:Importing libraries
2025-06-03 12:12:16,864:INFO:Copying training dataset
2025-06-03 12:12:16,914:INFO:Defining folds
2025-06-03 12:12:16,914:INFO:Declaring metric variables
2025-06-03 12:12:16,927:INFO:Importing untrained model
2025-06-03 12:12:16,935:INFO:Dummy Classifier Imported successfully
2025-06-03 12:12:16,966:INFO:Starting cross validation
2025-06-03 12:12:16,968:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-03 12:12:17,140:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,144:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,148:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,168:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,369:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,373:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,405:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,407:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,529:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,533:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-03 12:12:17,551:INFO:Calculating mean and std
2025-06-03 12:12:17,553:INFO:Creating metrics dataframe
2025-06-03 12:12:17,557:INFO:Uploading results into container
2025-06-03 12:12:17,559:INFO:Uploading model into container now
2025-06-03 12:12:17,559:INFO:_master_model_container: 14
2025-06-03 12:12:17,561:INFO:_display_container: 2
2025-06-03 12:12:17,561:INFO:DummyClassifier(constant=None, random_state=5898, strategy='prior')
2025-06-03 12:12:17,561:INFO:create_model() successfully completed......................................
2025-06-03 12:12:17,729:INFO:SubProcess create_model() end ==================================
2025-06-03 12:12:17,731:INFO:Creating metrics dataframe
2025-06-03 12:12:17,792:WARNING:C:\Users\Neville Evan\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-03 12:12:17,805:INFO:Initializing create_model()
2025-06-03 12:12:17,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002628B854BE0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-03 12:12:17,805:INFO:Checking exceptions
2025-06-03 12:12:17,821:INFO:Importing libraries
2025-06-03 12:12:17,821:INFO:Copying training dataset
2025-06-03 12:12:17,852:INFO:Defining folds
2025-06-03 12:12:17,860:INFO:Declaring metric variables
2025-06-03 12:12:17,860:INFO:Importing untrained model
2025-06-03 12:12:17,860:INFO:Declaring custom model
2025-06-03 12:12:17,860:INFO:K Neighbors Classifier Imported successfully
2025-06-03 12:12:17,860:INFO:Cross validation set to False
2025-06-03 12:12:17,860:INFO:Fitting Model
2025-06-03 12:12:17,900:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 12:12:17,900:INFO:create_model() successfully completed......................................
2025-06-03 12:12:18,122:INFO:_master_model_container: 14
2025-06-03 12:12:18,122:INFO:_display_container: 2
2025-06-03 12:12:18,122:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-03 12:12:18,122:INFO:compare_models() successfully completed......................................
